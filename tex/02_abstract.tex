%!TEX root = ../thesis.tex

\chapter*{Abstract}
\label{chap:abstract}
\addcontentsline{toc}{chapter}{\nameref{chap:abstract}}
The rapid advancement of intelligent systems and robotics has the potential to change our daily life and revolutionize various industries. A core aspect of these systems is the ability to accurately model and understand complex dynamic environments, enabling seamless and safe interactions with the real world. While significant progress has been made in image processing and language understanding through training on massive high-quality datasets, obtaining similar datasets for robotics is more challenging, costly, and time-consuming. This thesis addresses the issue of data scarcity using data-driven simulation, aiming to expand existing datasets by simulating or generating authentic variants to facilitate the training and testing of intelligent systems, ensuring their safe deployment.

We advance data-driven simulation of dynamic environments through two primary avenues: accurate estimation and efficient representation of scene dynamics, and neural scene reconstruction and simulation, with a specific focus on point cloud and LiDAR sensory data. First, we address the challenge of pairwise point cloud registration by proposing a novel overlap attention block, enhancing the robustness of registering point cloud pairs with low overlap. Next, we tackle scene dynamics estimation from multiple LiDAR frames by decomposing the scene into a static background and dynamic objects, parameterizing the scene dynamics with a set of rigid transformations, and performing motion estimation at the instance level. This results in highly accurate and efficient motion representation. Furthermore, we explore neural fields as an alternative to explicit mesh or surfel reconstructions for LiDAR simulation, proposing to augment the neural scene representation with a physical sensor model to produce realistic LiDAR simulations. Finally, we integrate the proposed motion representation with neural scene reconstruction tailored for LiDAR simulation, creating a versatile neural simulator that enables various scene editing capabilities. These contributions provide essential building blocks for developing robust and reliable autonomous systems, promising to transform multiple industries by enhancing efficiency, safety, and overall quality of life.


\chapter*{Kurzfassung}
\label{chap:kurzfassung}
\addcontentsline{toc}{chapter}{\nameref{chap:kurzfassung}}
\foreignlanguage{german}{
Die rasante Entwicklung intelligenter Systeme und der Robotik hat das Potenzial, das tägliche Leben und verschiedene Branchen zu revolutionieren. Ein zentraler Aspekt dieser Systeme ist die Fähigkeit, komplexe dynamische Umgebungen genau zu modellieren und zu verstehen, um nahtlose und sichere Interaktionen mit der realen Welt zu ermöglichen. Während im Bereich der Bildverarbeitung und des Sprachverständnisses durch das Training auf großen, qualitativ hochwertigen Datensätzen erhebliche Fortschritte erzielt wurden, ist die Beschaffung ähnlicher Datensätze für die Robotik schwieriger, kostspieliger und zeitaufwendiger. In dieser Arbeit wird das Problem der Datenknappheit mit Hilfe datengesteuerter Simulation angegangen. Ziel ist es, bestehende Datensätze durch Simulation oder Generierung authentischer Varianten zu erweitern, um das Training und Testen intelligenter Systeme zu erleichtern und deren sicheren Einsatz zu gewährleisten.

Wir fördern die datengesteuerte Simulation dynamischer Umgebungen durch zwei primäre Wege: genaue Schätzung und effiziente Darstellung der Szenendynamik sowie neuronale Szenenrekonstruktion und -simulation, mit besonderem Schwerpunkt auf Punktwolken und LiDAR-Sensordaten. Zunächst befassen wir uns mit der Herausforderung der paarweisen Registrierung von Punktwolken, indem wir einen neuartigen Überlappungs-Achtungsblock vorschlagen, der die Robustheit der Registrierung von Punktwolkenpaaren mit geringer Überlappung verbessert. Als Nächstes befassen wir uns mit der Schätzung der Szenendynamik aus mehreren LiDAR-Frames, indem wir die Szene in einen statischen Hintergrund und dynamische Objekte zerlegen, die Szenendynamik mit einer Reihe starrer Transformationen parametrisieren und die Bewegungsschätzung auf Instanzebene durchführen. Dies führt zu einer sehr genauen und effizienten Bewegungsdarstellung. Darüber hinaus untersuchen wir neuronale Felder als Alternative zu expliziten Mesh- oder Surfel-Rekonstruktionen für LiDAR-Simulationen und schlagen vor, die neuronale Szenendarstellung mit einem physikalischen Sensormodell zu ergänzen, um realistische LiDAR-Simulationen zu erzeugen. Schließlich integrieren wir die vorgeschlagene Bewegungsdarstellung mit einer auf LiDAR-Simulationen zugeschnittenen neuronalen Szenenrekonstruktion und schaffen so einen vielseitigen neuronalen Simulator, der verschiedene Möglichkeiten der Szenenbearbeitung bietet. Diese Beiträge liefern wesentliche Bausteine für die Entwicklung robuster und zuverlässiger autonomer Systeme, die durch die Verbesserung von Effizienz, Sicherheit und allgemeiner Lebensqualität einen Wandel in verschiedenen Branchen versprechen.

}