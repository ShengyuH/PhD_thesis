\chapter{Introduction}

\section{Motivation}

Imagine waking up to a world where a service robot has already prepared your cappuccino and croissant, and an autonomous vehicle is ready to smoothly take you to your office. Advances in robotics and autonomous systems have the potential to revolutionize various aspects of our lives and numerous industries, including healthcare, transportation, and manufacturing. Central to these innovations is the capability to accurately model and understand complex dynamic physical environments, allowing for seamless and safe interactions with the real world.

One key factor in creating intelligent systems is access to large, high-quality datasets. In image processing, pre-training on ImageNet~\cite{deng2009imagenet} has led to significant advancements in computer vision. Similarly, in language processing, large language models~\cite{radford2018improving} trained on extensive text datasets have shown superior capabilities across various tasks. These examples highlight the transformative impact of substantial, high-quality datasets on developing intelligent systems. However, obtaining such datasets for autonomous robots is challenging. Collecting real-world data is often costly, time-consuming, and comes with ethical and safety concerns, particularly in high-stakes domains like autonomous driving and service robotics. This data scarcity hinders the development and fine-tuning of algorithms needed for reliable performance in diverse and unpredictable environments.

Simulation offers a viable solution to address this dataset issue. It provides controlled and realistic environments for rigorous testing and training of these systems, especially for scenarios that are too rare or dangerous to replicate in real life. For example, in autonomous driving, simulations can replicate sudden pedestrian crossings or severe weather conditions, such as snowstorms and heavy rain. In healthcare, robots performing delicate surgeries can be trained in simulations to handle unexpected complications without risking patient safety. By simulating such scenarios, developers can ensure their systems are prepared for a wide range of conditions and challenges, enhancing the overall robustness and reliability of autonomous systems. Thus, simulation not only accelerates development but also improves the safety and effectiveness of deployed systems.

Traditional simulation approaches, such as those proposed in CARLA~\cite{dosovitskiy2017carla} and Isaac Sim~\cite{makoviychuk2021isaac}, have significantly advanced autonomous systems. However, these methods have several limitations. They are often costly and time-consuming to create, requiring extensive manual setup. This not only limits their scalability but also introduces a domain gap — the discrepancies between simulated environments and the real world. Such gaps can lead to underperformance when models trained in simulated settings are deployed in real-world scenarios. Therefore, while traditional simulations provide valuable insights and testing grounds, their limitations necessitate more efficient and scalable solutions to bridge the gap between simulation and reality.

An alternative to traditional simulation methods is data-driven simulation, which leverages sensory data captured from real environments. This approach involves reconstructing the environment from collected data and then simulating new sensory data by adjusting sensor configurations or modifying the environment itself. By utilizing real data, data-driven simulation significantly expands the coverage of training and testing datasets, providing more realistic and varied scenarios for system training. This approach bridges the gap between simulation and reality and is generally more cost-effective and scalable than traditional methods, making it a promising solution for developing robust and reliable autonomous systems.

The core of data-driven simulation consists of two main components: understanding and reconstructing dynamic scenes, and subsequently simulating new sensory data. Dynamic scenes are typically captured by moving robots and can often be decomposed into a few moving objects and a static background. Understanding the motion of the robot itself (ego-motion) and the motion of the dynamic parts from sensory data is essential for dynamic reconstruction, ensuring that the simulation reflects the true dynamics of the real world.

Once motion reconstruction is achieved, the next step involves simulating new sensory data by adjusting sensor configurations or modifying the scene dynamics. This capability allows for the creation of diverse and realistic training or testing scenarios. By simulating different viewpoints and conditions, this approach enhances the robustness and adaptability of the trained models, ultimately leading to more reliable autonomous systems.

In this thesis\footnote{Part of the text of this thesis was refined using ChatGPT, a language model developed by OpenAI.}, I aim to advance the field of data-driven simulation by addressing two key aspects: \textit{dynamic motion reconstruction} and \textit{neural scene reconstruction and simulation}, with a specific focus on point clouds as the sensory data. The following research questions are the focal points of my investigation.

\section{Research Questions}
\noindent
\textbf{\textit{Research Question 1: How to reliably and accurately recover ego-motion from point cloud fragments that are sparsely captured?}}

Point cloud registration has been a long-standing research question, essential for estimating more complex scene dynamics. Although recent works~\cite{gojcic2018learned,Choy2019FCGF} have made significant progress on traditional benchmarks like 3DMatch~\cite{zeng20163dmatch}, their performance in challenging low-overlap scenarios remains suboptimal. This is particularly critical since real-world scenes are often captured sparsely, making low-overlap a common issue. My goal is to develop a model that can robustly align point cloud fragments under low-overlap conditions.

\noindent
\textbf{\textit{Research Question 2: What is an effective motion representation for densely-captured dynamic street scenes, and how to accurately estimate it?}}

Street scenes are often captured by RGB cameras and LiDAR sensors at high frequencies, comprising a static background, parked vehicles, and moving agents such as vehicles, bicycles, and pedestrians. Understanding these scene dynamics is essential for reconstruction and subsequent simulation. Previous works~\cite{li2020neural,gojcic2021weakly} either use unconstrained scene flow representations that may produce physically implausible motions, or rely on only two frames, which do not account for the temporal smoothness of motions. The objective is to design a method that handles multiple frames and accurately predicts physically viable motions.

\noindent
\textbf{\textit{Research Question 3: What is a good neural scene representation and forward model for LiDAR simulation?}}

After addressing motion estimation for both sparsely captured static scenes and densely captured dynamic scenes, the next step is to design a scene representation and a forward model for simulation. Prior work~\cite{manivasagam2020lidarsim} employs explicit surface reconstruction and ray-surfel casting for LiDAR scans simulation. However, explicit reconstruction often results in the loss of surface details, and such decoupled two-stage methods cannot directly optimize the scene reconstruction for simulation tasks. The goal is to create an alternative neural scene representation and a forward model tailored to LiDAR sensors to faithfully reconstruct and simulate static LiDAR scans.

\noindent
\textbf{\textit{Research Question 4: How to simulate and edit LiDAR data in dynamic scenes?}}

Building on the neural scene representation and forward model developed for data-driven LiDAR simulation, the final step is to extend these methods to handle dynamic scenes. This involves integrating dynamic scene modeling, as discussed in Research Questions 1 and 2, with the neural scene representation and forward model from Research Question 3. Our aim is to develop a comprehensive framework that can flexibly edit dynamic scenes and authentically simulate LiDAR data in dynamic environments.

\section{Outline \& Contributions} 
This is a cumulative thesis comprising four scientific articles (\cf \cref{sec:pub}). For dynamic motion reconstruction, \cref{chap:cvpr21} first focuses on ego-motion estimation (pairwise point cloud registration) of static scenes, then \cref{chap:eccv22} proposes a motion representation for dynamic scenes as welll as how to estimate it, partly based on \cref{chap:cvpr21}. On the neural scene reconstruction and simulation side, \cref{chap:iccv23} proposes the neural scene representation and forward model for LiDAR simulation of static scenes; this, together with the motion representation proposed in \cref{chap:eccv22}, lay the foundation for LiDAR simulation of dynamic scenes as discussed in \cref{chap:cvpr24}. Speciﬁc contributions of each chapter are detailed as follows.

\subsection{Registration of 3D Point Clouds with Low Overlap}
In \cref{chap:cvpr21}, we address the challenge of pairwise registration of point clouds with low overlap by introducing \acro, a neural architecture designed to detect overlap regions between two unregistered scans and focus on those regions when sampling feature points. This approach results in improved registration recall. The main contributions of this work are as follows:

\begin{itemize}
\item We analyze the reasons why existing registration pipelines fail in low-overlap scenarios;
\item We propose a novel overlap attention block that facilitates early information exchange between the two point clouds and directs subsequent steps to focus on the overlap region;
\item We develop a scheme to refine feature point descriptors by conditioning them on the respective other point cloud;
\item We introduce a novel loss function to train matchability scores, which enhances the sampling of better and more repeatable interest points.
\end{itemize}

\subsection{Reconstruction of Multi-body Dynamics in Driving Scenes}
In ~\cref{chap:eccv22}, we explore the motion representation in street scenes and introduce a novel approach for multi-frame point cloud accumulation from LiDAR sequences. Our method leverages the inductive biases inherent in outdoor street scenes, such as their geometric layout and object-level rigidity, resulting in enhanced motion estimation. The main contributions of this work are as follows:

\begin{itemize}
\item We propose a novel learnable model for the temporal accumulation of 3D point cloud sequences across multiple frames, enabling improved reasoning about motion over extended time sequences;
\item Our model decomposes scenes into static background and agents that exhibit rigid motion over time, achieving holistic scene understanding, including foreground/background segmentation, motion segmentation, and per-object parametric motion compensation'
\item The use of Birds-Eye-View projection in our method facilitates low-latency processing;
\item We demonstrate the effectiveness of our approach in enhancing surface reconstruction.
\end{itemize}

\subsection{Neural LiDAR Fields for Novel View Synthesis}
In \cref{chap:iccv23}, we address the challenge of simulating LiDAR data from novel viewpoints or different sensor types. Unlike concurrent works~\cite{zhang2023nerflidar,tao2023lidarnerf} that treat LiDAR data merely as depth maps, we explore various sensor responses, including dropped rays, single returns, and dual returns. Compared to LiDARsim~\cite{manivasagam2020lidarsim}, our approach introduces an alternative neural scene representation, moving away from traditional explicit mesh or surface reconstructions. The contributions of this work are as follows:

\begin{itemize}
\item We devise a volume rendering technique specifically for LiDAR sensors;
\item We incorporate beam divergence into the forward model to accurately simulate dual returns;
\item We propose truncated volume rendering to account for secondary returns, thereby improving range prediction;
\item We develop a LiDAR simulator capable of synthesizing scenes from 3D assets, serving as a test bed for evaluating viewpoints far from the original scan locations and studying the effects of different scan patterns;
\item We introduce a novel closed-loop evaluation protocol that leverages real data to assess view synthesis in challenging perspectives.
\end{itemize}


\subsection{Dynamic LiDAR Simulation using Compositional Neural Fields}
In \cref{chap:cvpr24}, we present DyNFL, a novel approach for high-fidelity simulation of LiDAR scans in dynamic driving scenes. DyNFL constructs editable neural fields from LiDAR measurements in dynamic environments, enabling modifications such as changing viewpoints, adjusting object positions, and adding or removing objects. By leveraging scene decomposition and neural field composition techniques, DyNFL achieves high fidelity with flexible editing capabilities. The main contributions of this work are as follows:

\begin{itemize}
\item We introduce a framework for constructing editable neural fields from dynamic LiDAR measurements;
\item We propose a novel neural field composition technique for integrating assets reconstructed from other scenes;
\item We demonstrate the application of DyNFL in counterfactual scenario testing, providing a powerful tool for assessing the robustness of autonomous systems.
\end{itemize}

\section{List of Publications}
\label{sec:pub}

This thesis comprises the following four publications, for which I serve as the lead author\footnote{For P4, I proposed the project and hired Mr. Hanfeng Wu to work under my supervision. I led the design of the methodology and experimental setup, as well as contributed to the majority of the writing. * denotes equal contribution.}. For a detailed statement of my personal contributions to each individual paper, please refer to \cref{chap:contribution}. 
\begin{itemize}
    \item [P1] \noindent\textbf{ PREDATOR: Registration of 3D Point Clouds with Low Overlap} \\[0.5em]
    \textbf{Shengyu Huang}*, Zan Gojcic*, Mikhail Usvyatsov, Andreas Wieser, Konrad Schindler \\
    \textit{Conference on Computer Vision and Pattern Recognition (CVPR), 2021 (oral)}
    
    \item [P2] \noindent\textbf{ Dynamic 3D Scene Analysis by Point Cloud Accumulation} \\[0.5em]
    \textbf{Shengyu Huang}, Zan Gojcic, Jiahui Huang, Andreas Wieser, Konrad Schindler \\
    \textit{European Conference on Computer Vision (ECCV), 2022}
    
    \item [P3] \noindent\textbf{ Neural LiDAR Fields for Novel View Synthesis} \\[0.5em]
    \textbf{Shengyu Huang}, Zan Gojcic, Zian Wang, Francis William, Yoni Kasten, Sanja Fidler, Konrad Schindler, Or Litany \\
    \textit{International Conference on Computer Vision (ICCV), 2023}
    
    \item [P4] \noindent\textbf{ Dynamic LiDAR Re-simulation using Compositional Neural Fields} \\[0.5em]
    Hanfeng Wu, Xingxing Zuo, Stefan Leutenegger, Or Litany, Konrad Schindler, \textbf{Shengyu Huang} \\
    \textit{Conference on Computer Vision and Pattern Recognition (CVPR), 2024 (highlight)}
\end{itemize}

\vspace{1em}
\hrule
\vspace{1em}

\noindent
During my PhD, I also contributed to nine additional papers that are not included in this thesis; six have been published, and three are currently under submission. I have listed seven of these papers below, as they are highly relevant to the topics covered in this thesis. Please refer to \cref{chap:pub} for a full list of my publications.

\begin{itemize}   
    \item [P5] \noindent\textbf{ Neural Fields meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes} \\[0.5em]
    Zian Wang, Tianchang Shen, Jun Gao, \textbf{Shengyu Huang}, Jacob Munkberg, Jon Hasselgren, Zan Gojcic, Wenzheng Chen, Sanja Fidler \\
    \textit{Conference on Computer Vision and Pattern Recognition (CVPR), 2023}
    
    \item [P6] \noindent\textbf{ Living Scenes: Multi-object Relocalization and Reconstruction in Changing 3D Environments} \\[0.5em]
    Liyuan Zhu, \textbf{Shengyu Huang}, Konrad Schindler, Iro Armeni \\
    \textit{Conference on Computer Vision and Pattern Recognition (CVPR), 2024 (highlight)}

    \item [P7] \noindent\textbf{ LoopSplat: Gaussian Splatting Registration for Loop Closure} \\[0.5em]
    Liyuan Zhu, Yue Li, Erik Sandström, \textbf{Shengyu Huang}, Konrad Schindler, Iro Armeni \\
    \textit{International conference on 3D vision (3DV), 2025}

    \item [P8] \noindent\textbf{ ImpliCity: City Modeling from Satellite Images with Deep Implicit Occupancy Fields} \\[0.5em]
    Corrine Stucker, Bingxin Ke, Yuanwen Yue, \textbf{Shengyu Huang}, Iro Armeni, Konrad Schindler \\
    \textit{International Society for Photogrammetry and Remote Sensing (ISPRS) Congress, 2022 (Best Young Author Award)} 

    \item [P9] \noindent\textbf{ DeFlow: Self-supervised 3D Motion Estimation of Debris Flow} \\[0.5em]
    Liyuan Zhu, Yuru Jia, \textbf{Shengyu Huang}, Nicholas Meyer, Andreas Wieser, Konrad Schindler, Jordan Aaron \\
    \textit{Conference on Computer Vision and Pattern Recognition Workshop, 2023 (Best Paper Award)}

    \item [P10] \noindent\textbf{ Nothing Stands Still: A Spatiotemporal Benchmark on 3D Point Cloud Registration under Large Geometric and Temporal Change} \\[0.5em]
    Tao Sun, Yan Hao, \textbf{Shengyu Huang}, Silvio Savarese, Konrad Schindler, Marc Pollefeys, Iro Armeni \\
    \textit{in submission, 2024}

    \item [P11] \noindent\textbf{ Cross-Modal Feature Fusion for Registration of Point Clouds with Ambiguous Geometry} \\[0.5em]
    Zhaoyi Wang, \textbf{Shengyu Huang}, Jemil Butt, Yuanzhou Cai, Matej Varga, Andreas Wieser \\
    \textit{in submission, 2024}

\end{itemize}

\paragraph{Relevance to this thesis} For \textit{dyanmic motion reconstruction}, DeFlow~\cite{Zhu_2023_CVPR} addressed the challenge of estimating unconstrained and highly dynamic debris flow from RGB images and LiDAR scans; LivingScenes~\cite{zhu2023living} efficiently recovered multi-body dynamics of sparsely captured indoor scenes using \textbf{SE(3)}-equivariant representations; NothingStandStill~\cite{sun2023nothing} introduced a benchmark for spatio-temporal multi-way point cloud registration; LoopSplat~\cite{zhu2024_loopsplat} developed a dense RGB-D SLAM system that employs 3D Gaussian Splats as a unified scene representation for tracking, mapping, and maintaining global consistency, with its loop closure module enabled by our novel 3D Gaussian Splats registration technique. P11 enhanced RGB-D registration through a new feature fusion module. In the area of \textit{neural scene reconstruction and simulation}, ImpliCity~\cite{stucker2022implicity} proposed a neural field solution for city-scale reconstruction from satellite images. FEGR~\cite{wang2023fegr} (P5) combined neural fields with explicit meshes to accurately reconstruct scene geometry and recover intrinsic properties from posed images, enabling various downstream applications such as VR and AR where users may want to control the lighting of the environment and insert desired 3D objects into the scene.


\section{Relevance to Science and Economy}
\paragraph{Relevance to science}
The research presented in this thesis has wide-reaching implications across multiple disciplines. The advancements in point cloud registration and motion estimation address the long-standing correspondence problems in computer vision, with the developed techniques being adopted in various fields, including robotics~\cite{wen2024foundationpose}, autonomous driving~\cite{seidenschwarz2024semoli}, AR/VR~\cite{Zhao_2023_ICCV}, photogrammetry~\cite{xu2023point_isprs}, and building information modeling~\cite{rashdi2022scanning}. The development of neural reconstruction techniques for LiDAR simulation advances data-driven neural simulation, offering a viable solution to the issue of data scarcity in developing intelligent systems and robotics. Additionally, we emphasize the importance of accurate sensor modelling~\cite{ehret2024radar,klinghoffer2024platonerf} when addressing various inverse problems using neural fields, which inspired us to organize the first workshop on Neural Fields Beyond Conventional Cameras~\cite{nfcc} at the upcoming ECCV 2024.


\paragraph{Scientific recognition}
The content presented in this thesis is derived from thirteen papers I produced during my PhD, four of which are included in this thesis. Ten of these papers were published at renowned conferences in Computer Vision, Robotics, or Remote Sensing. Several of them received recognitions such as Best Young Author Award (P8), Best Paper Award (P5), Best Paper Award Candidate~\cite{ke2023marigold}, oral (P1, ~\cite{ke2023marigold}), or highlight papers(P4, P6). Additionally, ten out of the thirteen projects were open-sourced and received recognition from the community in the form of github stars. The datasets, code, and accompanying illustrative videos are available on my personal website: \url{shengyuh.github.io}.

\paragraph{Relevance to economy}
The advancements in data-driven simulation presented in this thesis have significant economic implications, particularly for the rapidly growing fields of automation and autonomous robotics.

In the automotive industry, enhanced simulation techniques can accelerate the development and deployment of autonomous vehicles by reducing reliance on physical prototyping and extensive road testing. This can lead to lower development costs and faster integration of autonomous vehicles into transportation systems, potentially improving traffic efficiency and reducing accidents.

For service robots, improved simulation tools enable the creation of more reliable robots capable of operating in various environments, from homes to hospitals. By automating repetitive and labor-intensive tasks through intelligent robots, our society can reduce labor costs and enhance productivity across multiple service sectors, ultimately contributing to economic growth and improved quality of life.

In summary, the research on data-driven neural simulation in this thesis supports the progress of automation and autonomous robotics, offering potential economic benefits and influencing the future of various industries.