\chapter{Introduction}

\section{Motivation}

Imagine waking up to a world where a service robot has already prepared your cappuccino and croissant, and an autonomous vehicle is ready to smoothly take you to your office. Advances in robotics and autonomous systems have the potential to revolutionize various aspects of our lives and numerous industries, including healthcare, transportation, and manufacturing. Central to these innovations is the capability to accurately model and understand complex dynamic physical environments, allowing for seamless and safe interactions with the real world.

Simulation plays a critical role in the development of these intelligent systems. By providing controlled and authentic environments, simulations allow these systems to be rigorously tested and trained. This approach is particularly important for exploring scenarios that are either too rare or too dangerous to reproduce in real life. For instance, in autonomous driving, simulating scenarios such as sudden pedestrian crossings or severe weather conditions like snowstorms and heavy rain is crucial. Similarly, in healthcare, robots performing delicate surgeries can be trained in simulations to handle unexpected complications without risking patient safety. By simulating such scenarios, developers can ensure their systems can handle a wide range of conditions and challenges, thereby enhancing the overall robustness and reliability of autonomous systems. Thus, simulation not only accelerates the development process but also contributes to the safety and effectiveness of the deployed systems.

Traditional simulation approaches, such as those proposed in CARLA~\cite{dosovitskiy2017carla} and Isaac Sim~\cite{makoviychuk2021isaac}, have significantly advanced autonomous systems. However, these methods have several limitations. They are often costly and time-consuming to create, requiring extensive manual setup. This not only limits their scalability but also introduces a domain gap—the discrepancies between simulated environments and the real world. Such gaps can lead to underperformance when models trained in simulated settings are deployed in real-world scenarios. Therefore, while traditional simulations provide valuable insights and testing grounds, their limitations necessitate more efficient and scalable solutions to bridge the gap between simulation and reality.

An alternative to traditional simulation methods is data-driven simulation, which leverages sensory data captured from real environments. This approach involves reconstructing the environment from collected data and then simulating new sensory data by adjusting sensor configurations or modifying the environment itself. By utilizing real data, data-driven simulation significantly expands the coverage of training and testing datasets, providing more realistic and varied scenarios for system training. This approach bridges the gap between simulation and reality and is generally more cost-effective and scalable than traditional methods, making it a promising solution for developing robust and reliable autonomous systems.

The core of data-driven simulation consists of two main components: understanding and reconstructing dynamic scenes, and subsequently simulating new sensory data. Dynamic scenes are typically captured by moving robots and can often be decomposed into a few moving objects and a static background. Understanding the motion of the robot itself (ego-motion) and the motion of the dynamic parts from sensory data is essential for dynamic reconstruction, ensuring that the simulation reflects the true dynamics of the real world.

Once motion reconstruction is achieved, the next step involves simulating new sensory data by adjusting sensor configurations or modifying the scene dynamics. This capability allows for the creation of diverse and realistic training or testing scenarios. By simulating different viewpoints and conditions, this approach enhances the robustness and adaptability of the trained models, ultimately leading to more reliable autonomous systems.

In this thesis, we aim to advance the field of data-driven simulation by addressing two key aspects: dynamic scene reconstruction and simulation, with a specific focus on point clouds as the sensory data. The following research questions are the focal points of our investigation.

\section{Research Questions}

\noindent
\textbf{\textit{Research Question 1: How can we reliably and accurately recover ego-motion from point cloud fragments that are sparsely captured?}}

Point cloud registration has been a long-standing research question. Although recent works~\cite{gojcic2018learned,Choy2019FCGF} have made significant progress on traditional benchmarks like 3DMatch~\cite{zeng20163dmatch}, their performance in challenging low-overlap scenarios remains suboptimal. This is particularly critical since real-world scenes are often captured sparsely, making low-overlap a common issue. Our goal is to develop a model that can robustly align point cloud fragments under low-overlap conditions.

\noindent
\textbf{\textit{Research Question 2: What is an effective motion representation for densely-captured dynamic street scenes, and how can we accurately estimate it?}}

Street scenes are often captured by RGB cameras and LiDAR sensors at high frequencies, comprising a static background, parked vehicles, and moving agents such as vehicles, bicycles, and pedestrians. Understanding these scene dynamics is essential for reconstruction and subsequent simulation. Previous works~\cite{li2020neural,gojcic2021weakly} either use unconstrained scene flow representations that may produce physically implausible motions or rely on only two frames, which do not account for the temporal smoothness of motions. Our objective is to design a method that handles multiple frames and accurately predicts physically viable motions.

\noindent
\textbf{\textit{Research Question 3: What is an appropriate neural scene representation and forward model for LiDAR simulation?}}

After addressing motion estimation for both sparsely captured static scenes and densely captured dynamic scenes, the next step is to design a scene representation and a forward model for simulation. Prior work~\cite{manivasagam2020lidarsim} employs explicit surface reconstruction and ray-surfel casting for LiDAR scans simulation. However, explicit reconstruction often results in the loss of surface details, and such decoupled two-stage methods cannot directly optimize the scene reconstruction for simulation tasks. Our goal is to create an alternative neural scene representation and a forward model tailored to LiDAR sensors to faithfully reconstruct and simulate static LiDAR scans.

\noindent
\textbf{\textit{Research Question 4: How can we simulate and edit LiDAR data in dynamic scenes?}}

Building on the neural scene representation and forward model developed for data-driven LiDAR simulation, the final step is to extend these methods to handle dynamic scenes. This involves integrating dynamic scene modeling, as discussed in Research Questions 1 and 2, with the neural scene representation and forward model from Research Question 3. Our aim is to develop a comprehensive framework that can flexibly edit dynamic scenes and authentically simulate LiDAR data in dynamic environments.

\section{Outline \& Contributions} 
This thesis addresses the research questions outlined earlier and contributes to the instigation of dynamic scene reconstruction and simulation. Speciﬁc contributions are detailed as follows.


\subsection{Registration of 3D Point Clouds with Low Overlap}
In \cref{chap:cvpr21}, we address the challenge of pairwise registration of point clouds with low overlap by introducing \acro, a neural architecture designed to detect overlap regions between two unregistered scans and focus on those regions when sampling feature points. This approach results in improved registration recall. The main contributions of this work are as follows:

\begin{itemize}
\item We analyze the reasons why existing registration pipelines fail in low-overlap scenarios.
\item We propose a novel overlap attention block that facilitates early information exchange between the two point clouds and directs subsequent steps to focus on the overlap region.
\item We develop a scheme to refine feature point descriptors by conditioning them on the respective other point cloud.
\item We introduce a novel loss function to train matchability scores, which enhances the sampling of better and more repeatable interest points.
\end{itemize}

\subsection{Reconstruction of Multi-body Dynamics in Driving Scenes}
In ~\cref{chap:eccv22}, we explore the motion representation in street scenes and introduce a novel approach for multi-frame point cloud accumulation from LiDAR sequences. Our method leverages the inductive biases inherent in outdoor street scenes, such as their geometric layout and object-level rigidity, resulting in enhanced motion estimation. The main contributions of this work are as follows:

\begin{itemize}
\item We propose a novel learnable model for the temporal accumulation of 3D point cloud sequences across multiple frames, enabling improved reasoning about motion over extended time sequences.
\item Our model decomposes scenes into static background and agents that exhibit rigid motion over time, achieving holistic scene understanding, including foreground/background segmentation, motion segmentation, and per-object parametric motion compensation.
\item The use of Birds-Eye-View projection in our method facilitates low-latency processing.
\item We demonstrate the effectiveness of our approach in enhancing surface reconstruction.
\end{itemize}

\subsection{Neural LiDAR Fields for Novel View Synthesis}
In \cref{chap:iccv23}, we address the challenge of simulating LiDAR data from novel viewpoints or different sensor types. Unlike concurrent works~\cite{zhang2023nerflidar,tao2023lidarnerf} that treat LiDAR data merely as depth maps, we explore various sensor responses, including dropped rays, single returns, and dual returns. Compared to LiDARsim~\cite{manivasagam2020lidarsim}, our approach introduces an alternative neural scene representation, moving away from traditional explicit mesh or surface reconstructions. The contributions of this work are as follows:

\begin{itemize}
\item We devise a volume rendering technique specifically for LiDAR sensors.
\item We incorporate beam divergence into the forward model to accurately simulate dual returns.
\item We propose truncated volume rendering to account for secondary returns, thereby improving range prediction.
\item We develop a LiDAR simulator capable of synthesizing scenes from 3D assets, serving as a test bed for evaluating viewpoints far from the original scan locations and studying the effects of different scan patterns.
\item We introduce a novel closed-loop evaluation protocol that leverages real data to assess view synthesis in challenging perspectives.
\end{itemize}


\subsection{Dynamic LiDAR Simulation using Compositional Neural Fields}
In \cref{chap:cvpr24}, we present DyNFL, a novel approach for high-fidelity simulation of LiDAR scans in dynamic driving scenes. DyNFL constructs editable neural fields from LiDAR measurements in dynamic environments, enabling modifications such as changing viewpoints, adjusting object positions, and adding or removing objects. By leveraging scene decomposition and neural field composition techniques, DyNFL achieves high fidelity with flexible editing capabilities. The main contributions of this work are as follows:

\begin{itemize}
\item We introduce a framework for constructing editable neural fields from dynamic LiDAR measurements.
\item We propose a novel neural field composition technique for integrating assets reconstructed from other scenes.
\item We demonstrate the application of DyNFL in counterfactual scenario testing, providing a powerful tool for assessing the robustness of autonomous systems.
\end{itemize}

\section{List of Publications}

This thesis includes the following 4 publications:
\begin{itemize}
    \item \noindent\textbf{ PREDATOR: Registration of 3D Point Clouds with Low Overlap} \\[0.5em]
    \textbf{Shengyu Huang}*, Zan Gojcic*, Mikhail Usvyatsov, Andreas Wieser, Konrad Schindler \\
    \textit{Conference on Computer Vision and Pattern Recognition (CVPR), 2021 (Oral)}
    
    \item \noindent\textbf{ Dynamic 3D Scene Analysis by Point Cloud Accumulation} \\[0.5em]
    \textbf{Shengyu Huang}, Zan Gojcic, J. Huang, Andreas Wieser, Konrad Schindler \\
    \textit{European Conference on Computer Vision (ECCV), 2022}
    
    \item \noindent\textbf{ Neural LiDAR Fields for Novel View Synthesis} \\[0.5em]
    \textbf{Shengyu Huang}, Zan Gojcic, Zian Wang, F. William, Y. Kasten, S. Fidler, Konrad Schindler, O. Litany \\
    \textit{International Conference on Computer Vision (ICCV), 2023}
    
    \item \noindent\textbf{ Dynamic LiDAR Re-simulation using Compositional Neural Fields} \\[0.5em]
    Hanfeng Wu, Xingxing Zuo, Stefan Leutenegger, Or Litany, Konrad Schindler, \textbf{Shengyu Huang} \\
    \textit{Conference on Computer Vision and Pattern Recognition (CVPR), 2024 (Highlight)}
\end{itemize}

\vspace{1em}
\hrule
\vspace{1em}

\noindent
9 papers that I contributed to during my PhD but are not included in this thesis, 6 published papers and 3 papers in submission:

\begin{itemize}    
    \item \noindent\textbf{ ImpliCity: City Modeling from Satellite Images with Deep Implicit Occupancy Fields} \\[0.5em]
    Corrine Stucker, Bingxin Ke, Yuanwen Yue, \textbf{Shengyu Huang}, Iro Armeni, Konrad Schindler \\
    \textit{International Society for Photogrammetry and Remote Sensing (ISPRS) Congress, 2022 (Best Young Author Award)}
    
    \item \noindent\textbf{ DeFlow: Self-supervised 3D Motion Estimation of Debris Flow} \\[0.5em]
    Liyuan Zhu, Yuru Jia, \textbf{Shengyu Huang}, Nicholas Meyer, Andreas Wieser, Konrad Schindler, Jordan Aaron \\
    \textit{CVPR Workshop, 2023 (Best Paper Award)}
    
    \item \noindent\textbf{ Neural Fields meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes} \\[0.5em]
    Zian Wang, Tianchang Shen, Jun Gao, \textbf{Shengyu Huang}, Jacob Munkberg, Jon Hasselgren, Zan Gojcic, Wenzheng Chen, Sanja Fidler \\
    \textit{Conference on Computer Vision and Pattern Recognition (CVPR), 2023}
    
    \item \noindent\textbf{ Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation} \\[0.5em]
    Bingxin Ke, Anton Obukhov, \textbf{Shengyu Huang}, Nando Metzger, Rodrigo Daudt, Konrad Schindler \\
    \textit{Conference on Computer Vision and Pattern Recognition (CVPR), 2024 (Oral, Best Paper Award Candidate)}
    
    \item \noindent\textbf{ Living Scenes: Multi-object Relocalization and Reconstruction in Changing 3D Environments} \\[0.5em]
    Liyuan Zhu, \textbf{Shengyu Huang}, Konrad Schindler, Iro Armeni \\
    \textit{Conference on Computer Vision and Pattern Recognition (CVPR), 2024 (Highlight)}
    
    \item \noindent\textbf{ DGInStyle: Domain Generalizable Semantic Segmentation with Image Diffusion Models and Stylized Semantic Control} \\[0.5em]
    Yuru Jia, Lukas Hoyer, \textbf{Shengyu Huang}, Tianfu Wang, Luc Van Gool, Konrad Schindler, Anton Obukhov \\
    \textit{European Conference on Computer Vision (ECCV), 2024}
    
    \item \noindent\textbf{ Nothing Stands Still: A Spatiotemporal Benchmark on 3D Point Cloud Registration under Large Geometric and Temporal Change} \\[0.5em]
    Tao Sun, Yan Hao, \textbf{Shengyu Huang}, Silvio Savarese, Konrad Schindler, Marc Pollefeys, Iro Armeni \\
    \textit{arXiv, 2023}
    
    \item \noindent\textbf{ Cross-Modal Feature Fusion for Registration of Point Clouds with Ambiguous Geometry} \\[0.5em]
    Zhaoyi Wang, \textbf{Shengyu Huang}, Jemil Butt, Yuanzhou Cai, Matej Varga, Andreas Wieser \\
    \textit{in submission, 2024}

    \item \noindent\textbf{ LoopSplat: Gaussian Splatting Registration for SLAM Loop Closure} \\[0.5em]
    Liyuan Zhu, Yue Li, Erik Sandström, \textbf{Shengyu Huang}, Konrad Schindler, Iro Armeni \\
    \textit{in submission, 2024}

\end{itemize}




\section{Relevance to Science and Economy}
\paragraph{Scientific Recognition}
The content presented in this thesis is derived from twelve papers I produced during my PhD, four of which are included in this thesis. Ten of these papers were published at renowned conferences in Computer Vision, Robotics, or Remote Sensing. Several of them received recognitions such as Best Paper Award, Best Paper Award Candidate, Oral, or Highlight papers. One paper was accepted at a workshop and won the Best Paper Award. Additionally, ten out of the twelve projects were open-sourced and received recognition from the community in the form of github stars. The datasets, code, and accompanying illustrative videos are available on my personal website: \url{shengyuh.github.io}.

\paragraph{Relevance to Economy}
The advancements in data-driven simulation and autonomous systems detailed in this thesis have significant economic implications, particularly for the automotive and service robotics industries. Enhanced simulation capabilities can accelerate the development and deployment of autonomous vehicles, reducing costs associated with physical prototyping and extensive road testing. This facilitates faster and safer integration of autonomous cars into transportation networks, potentially transforming infrastructure, improving traffic efficiency, and reducing accidents.

For service robots, improved simulation tools enable the creation of more reliable and versatile robots capable of performing tasks in various environments, from homes to hospitals. This can lead to significant labor cost reductions and increased productivity in service sectors. By automating repetitive tasks, these advanced autonomous systems promise to enhance quality of life and drive economic growth by streamlining daily tasks and professional services.