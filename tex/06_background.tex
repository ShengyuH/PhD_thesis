\chapter{Background}
\label{chap:background}
In this chapter, we provide an overview of the research fields relevant to this thesis. Specifically, we focus on two aspects: point cloud processing~\cref{sec:bg_point_cloud} and neural fields in visual computing~\cref{sec:bg_neural_field}. Detailed related works for each specific task are discussed in subsequent chapters.

\section{Point Cloud Processing}
\label{sec:bg_point_cloud}
3D data can be represented in various forms, including multi-view images~\cite{su2015multi,qi2016volumetric}, voxels~\cite{wu20153d,maturana2015voxnet}, point clouds~\cite{qi2017pointnet,wang2019dynamic}, polygon meshes~\cite{hanocka2019meshcnn,NEURIPS2020_0a656cc1}, and implicit representations~\cite{mescheder2019occupancy,park2019deepsdf,chen2019learning,yang2021geometry}. Among these, point clouds are favored for their scalability and reduced complexity compared to voxels and meshes, and their explicitness compared to multi-view images and implicit representations. However, point cloud data is unordered and unstructured, requiring models to be invariant to $N!$ permutations and robust to \textbf{SE}(3) transformations. Additionally, point clouds often have varied point density and noise, posing challenges for learning-based models. In this section, we introduce various learning-based methods that handle unstructured point cloud data. For a comprehensive review, refer to Guo \etal~\cite{guo2020deep}.

\paragraph{Point-wise MLP}
DeepSets~\cite{zaheer2017deep} and PointNet~\cite{qi2017pointnet} pioneered deep learning for point cloud processing by introducing permutation-invariant operators. PointNet processes each point individually using shared MLPs, followed by global max pooling to aggregate features, capturing global point cloud characteristics while being invariant to permutations. DeepSets~\cite{zaheer2017deep} achieves permutation invariance by summing all representations and applying nonlinear transformations. PointNet++\cite{qi2017pointnet++} extends PointNet by incorporating hierarchical structures to capture local context and progressively enrich global features. These methods are simple and effective for handling unordered point clouds but may struggle with fine-grained local structures due to the pooling approach. Extensions of PointNet++ improve local structure capture through enhanced neighborhood selection and aggregation techniques~\cite{wang2019dynamic}, as well as scalability~\cite{NEURIPS2022_9318763d}. RandLA-Net\cite{hu2020randla} focuses on real-time large-scale point cloud processing with a random sampling strategy and lightweight local feature aggregation, reducing computational load while maintaining performance.

\paragraph{Point Convolution}
To improve the efficiency of local feature aggregation, point convolution methods~\cite{li2018pointcnn,wu2019pointconv,thomas2019kpconv} define convolution kernels for point cloud data, with weights for neighboring points related to their spatial distribution relative to the center point. KPConv~\cite{thomas2019kpconv} introduces kernel points and learns weights for these points to perform convolutions directly on point sets. For varied density or complex geometry, deformable kernel points with learnable shifts are proposed. Compared to point-wise MLP-based methods~\cite{qi2017pointnet,qi2017pointnet++}, KPConv excels in detailed local feature extraction for tasks such as 3D semantic segmentation and representation learning~\cite{bai2020d3feat}. However, tuning the receptive field for different point densities and handling noise can be challenging.

\paragraph{Sparse Convolution}
Addressing the inherent sparsity of point cloud data, sparse convolution methods~\cite{tang2022torchsparse,hackel2020inference,3DSemanticSegmentationWithSubmanifoldSparseConvNet} focus on non-empty voxels and preserve sparse structures in deep layers, significantly reducing computational load and memory usage. These methods utilize hash tables to efficiently access active voxels, making them highly efficient for large-scale 3D scene understanding. Compared to point convolution methods, the receptive fields of sparse convolution layers are more intuitive and easier to understand. However, voxelization can lead to loss of fine detail and precision, which may be critical for fine-grained recognition tasks.

\paragraph{BEV Projection}
Bird's Eye View (BEV) projection transforms point clouds into a top-down 2D grid representation, simplifying the data structure and making it compatible with traditional 2D convolutional neural networks. This technique is widely used in autonomous driving for tasks like object detection~\cite{zhou2018voxelnet,lang2019pointpillars} and semantic segmentation~\cite{aksoy2020salsanet}, leveraging efficient 2D CNN architectures. The main advantage is computational efficiency and ease of integration with existing 2D CNN frameworks. However, the projection can lead to loss of height information and resolution, potentially affecting 3D feature extraction accuracy. Advanced approaches combine BEV with other local feature aggregations. PointPillars~\cite{lang2019pointpillars} applies PointNet~\cite{qi2017pointnet} within each grid to aggregate local context before applying 2D operations.

\section{Neural field in visual computing}
\label{sec:bg_neural_field}
Besides explicit representations like point cloud, voxel, mesh, and multi-view images, 3D data can also be represented implicitly, through fully-connected neural network weights. Explicit representations suffer from loss of details due to discretisation. Neural fields represent the shape as a continuous filed 
 DeepSDF~\cite{park2019deepsdf}, Occupancy network~\cite{mescheder2019occupancy}, and IM-NET~\cite{chen2019learning} poineered representing 3D data implicitly in function space. NeRF