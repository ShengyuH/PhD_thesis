\chapter{Conclusions}
\label{chap:conclusion}

\section{Core contributions \& Applications}
This thesis advances the field of data-driven neural simulation for intelligent systems by innovating in two key areas: the representation and estimation of scene dynamics, and neural scene reconstruction.

~\cref{chap:cvpr21} makes a significant advancement in the long-standing challenge of point cloud registration by addressing the difficult task of registering point cloud pairs with low overlap. Through a careful examination of the failure cases of existing methods, we introduce the concept of the overlap region at the keypoint sampling step, proposing to extract such contextual information through Transformer architecture~\cite{vaswani2017attention} in the UNet bottleneck, thereby significantly enhancing the reliability of registration models with the novel overlap attention block. The simplicity and effectiveness of this method have led to its widespread adoption in point cloud registration. For example, \cite{qin2023geotransformer,yu2021cofinet} incorporate it into the coarse-to-fine point cloud registration pipeline to improve accuracy, while \cite{attaiki2021dpfm,li2022lepard} extend the overlap attention block to non-rigid registration methods. Zhang \etal~\cite{zhang2022pcr} utilize it for RGB-D registration tasks. Additionally, \cite{delitzas2024scenefun3d} employs it to construct datasets for scene understanding tasks, and \cite{wen2024foundationpose} demonstrates its potential in 6 DoF pose estimation.

~\cref{chap:iccv23} presents a powerful neural scene reconstruction method for data-driven LiDAR simulation. This approach combines the superior representation capabilities of neural fields with a detailed, physical sensor model, enabling authentic LiDAR simulations from novel viewpoints and under different sensor configurations. By carefully modeling various sensor responses, we have significantly enhanced the realism of simulated LiDAR scans, facilitating their adoption for closed-loop testing of autonomous navigation systems. This work has inspired subsequent research on LiDAR 4D reconstruction, such as \cite{zheng2024lidar4d,zhong20243d}. Additionally, NFL emphasizes the importance of an accurate sensor model and corresponding forward model when addressing various inverse problems using neural fields, as also highlighted by \cite{ehret2024radar,klinghoffer2024platonerf}. This research also led to my co-organization of the first workshop on "Neural Fields Beyond Conventional Cameras" at ECCV 2024.

\acro~and NFL, detailed in \cref{chap:cvpr21} and \cref{chap:iccv23},  address the challenging tasks of motion estimation and neural reconstruction in static scenes. However, our world is predominantly dynamic. PCAccumlation, discussed in \cref{chap:eccv22}, introduces an efficient representation of scene dynamics by decomposing the scene into a static background and a set of rigidly moving agents, representing their motions using compact \textbf{SE(3)} transformations. By sequentially performing foreground-background segmentation, motion segmentation, instance association, and motion regression, we not only achieve highly accurate motion estimation but also develop a holistic understanding of the scenes. The efficiency of such motion representations has also been validated in~\cite{seidenschwarz2024semoli,vidanapathirana2024multi}. Finally, in \cref{chap:cvpr24}, we combine this motion representation with neural scene representation tailored for LiDAR simulation, creating a neural simulator that enables various scene editing capabilities, marking a significant step towards data-driven neural simulation.

Beyond its primary focus on data-driven neural simulation, this thesis also offers valuable insights into two fundamental research questions in 3D vision: \textit{correspondence} and \textit{representation}.

When asked, "What are the three most important problems in computer vision?", the renowned computer vision scientist Takeo Kanade famously responded, "Correspondence, correspondence, correspondence!"~\cite{Wang-2019-117586} Indeed, correspondence problems are fundamental in computer vision as they are crucial for various applications, including 3D reconstruction~\cite{schoenberger2016mvs,schoenberger2016sfm}, optical flow estimation~\cite{dosovitskiy2015flownet,sun2018pwc}, and object recognition and tracking~\cite{tang2023emergent}, among others. My thesis addresses this critical challenge from multiple angles. \cref{chap:cvpr21} focuses on the geometric point correspondence problems in 3D. Rather than merely improving local feature descriptors, we introduce an important concept in feature matching: keypoints should only be sampled for the matching task if they are in the overlap region. The proposed overlap attention module significantly enhances the robustness of registration methods and complements advancements in point cloud representation learning. In \cref{chap:eccv22}, we tackle object correspondence problems using sequential LiDAR scans. Our method robustly associates objects across time, despite their partial visibility, through accurate segmentation and clustering. 

Another foundamental challenge of 3D vision research is representation: what is the most efficient representation for a given task? My research presented in this thesis provides several insights and solutions. \cref{chap:cvpr21} delves into rich geometric and semantic features in a high-dimensional latent space, subsequently decoding them into overlap scores to improve keypoint detection. \cref{chap:eccv22} proposes representing street scene dynamics using compact \textbf{SE(3)} transformations, overcoming the limitations of unconstrained scene flow representations~\cite{wu2019pointpwc}. Furthermore, \cref{chap:iccv23} introduces a novel neural scene representation for data-driven LiDAR simulation. This approach offers an alternative to explicit mesh or surfel representations, which often lose detail when modeling complex geometry. By combining the proposed motion representation with the scene representation, \cref{chap:cvpr24} constructs a flexible and powerful neural simulator, advancing the field of data-driven neural simulation.


\section{Outlook}
The field of neural reconstruction and simulation of dynamic environments has seen significant progress thanks to recent advancements in neural scene representation and vision foundation models. In this section, beyond the limitations and future work discussed in each individual chapter, I outline some compelling future directions in this domain.


\paragraph{Feature matching meets regression for pose estimation}
\acro~\cite{huang2021predator} significantly enhances the robustness of point cloud registration; however, there are still scenarios where it may fail. For instance, if the spatial extent of the overlapping region is very small and key points are too close to each other, the method may struggle to accurately recover the relative pose. In some cases, there may even be no overlap at all~\cite{xu2023point}, necessitating prior knowledge of the shape or scene to be reconstructed. Recently, DUST3R~\cite{wang2024dust3r,leroy2024grounding} demonstrated that an aligned point cloud can be directly regressed from two unposed images, and the pose can subsequently be estimated directly between one point cloud and its rotated variant, overcoming the aforementioned issue caused by small or zero overlap. The key to this success lies in the multi-view pre-training of the encoder and large-scale training on mixed datasets, which ideally cover a diverse range of scene geometries and relative camera poses. It would be interesting to explore a similar paradigm for point cloud registration through joint training on large mixed datasets.


\paragraph{Point cloud base model}
2D image processing has significantly benefited from self-supervised pretraining on large datasets since the rise of deep learning. However, pretraining on 3D data has lagged behind for several reasons. Firstly, there is a scarcity of large, high-quality 3D datasets. The largest dataset currently available, Objaverse-XL~\cite{deitke2024objaverse}, remains much smaller than the largest text-image dataset, Laion~\cite{schuhmann2022laion}. Additionally, Objaverse-XL is object-centric and uses mesh representation, where sampling point clouds from these meshes can introduce biased local patterns. It remains an open question how large a 3D dataset needs to be and what its distribution should look like, including the proportions of object-centric, indoor, and outdoor scenes, for large scale pre-training. 

Secondly, there is no standard backbone for point cloud processing. Different backbones are utilized due to their specific advantages and disadvantages, tailored to user needs. While various techniques are discussed in \cref{chap:background}, point transformers~\cite{zhao2021point,wu2022point,wu2024point} have recently gained popularity for their simplicity and efficiency. Notably, PointTransformerV3~\cite{wu2024point} demonstrates superior speed and memory efficiency during both training and inference compared to sparse convolution~\cite{choy2019Minkowski}, highlighting its potential for large-scale pretraining. In the long term, it is crucial for the community to reach a consensus on a standard backbone to advance point cloud pretraining.

The third challenge lies in the pretraining paradigm. Large-scale pretraining often involves different datasets with varying annotations and point patterns, necessitating an efficient pretraining approach to handle this diversity. Recently, PointPrompt Training~\cite{wu2024towards} addressed the challenge of negative transfer across 3D datasets by introducing a framework that adapts models to different datasets using domain-specific prompts and aligns label spaces with language-guided categorical alignment. This approach enables a single, weight-shared model to achieve state-of-the-art performance across multiple 3D point cloud datasets, overcoming the limitations of mixed supervision and enhancing representation quality for diverse downstream tasks. However, this method is currently limited to training with language guidance across synthetic and real datasets. It would be beneficial to explore more generic pretraining strategies, such as reconstruction~\cite{xiao2013sun3d} and training across all available datasets.


\paragraph{Generalisable neural LiDAR fields}
NFL and DyNFL~\cite{Huang2023nfl,Wu2023dynfl} have primarily focused on using simulated LiDAR scans for closed-loop testing of autonomous driving systems. However, their application to training perception models remains limited due to the slow and computationally expensive per-scene optimization process. In contrast, their 2D counterparts, such as generalizable NeRF~\cite{yu2021pixelnerf,chen2021mvsnerf}, have demonstrated superiority in sparse view settings by leveraging generalizable image features and monocular priors. These methods can produce high-quality novel view synthesis even from previously unobserved viewpoints without the need for expensive per-scene optimization. Therefore, it would be intriguing to explore generalizable neural LiDAR fields that can utilize generalizable point features and scene layout priors for fast and high-quality LiDAR novel view synthesis. Due to occlusions, many parts of street scenes are only observable from certain viewpoints, making scene completion even more critical.