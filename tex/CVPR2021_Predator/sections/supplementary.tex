\section{Appendix}
In this appendix, we first provide rigorous definitions of evaluation metrics (Sec.~\ref{sec:evaluation_metrics_supp}), then describe the data pre-processing step (Sec.~\ref{sec:datasets_supp}), network architectures (Sec.~\ref{sec:network_arch_supp}) and training on individual datasets (Sec.~\ref{sec:training_supp}) in more detail. We further provide additional results (Sec.~\ref{sec:additional_results_supp}), ablation studies (Sec.~\ref{sec:addtional_ablation_supp}) as well as a runtime analysis (Sec.~\ref{sec:timing}). Finally, we show more visualisations on \emph{3DLoMatch} and \emph{ModelLoNet} benchmarks (Sec.~\ref{sec:qualitative_supp}). 


\subsection{Evaluation metrics}
\label{sec:evaluation_metrics_supp}
The evaluation metrics, which we use to assess model performance in Sec.~4 of the main paper and Sec.~\ref{sec:additional_results_supp} of this appendix, are formally defined as follows:

\textbf{Inlier ratio} looks at the set of putative correspondences $(\mathbf{p}, \mathbf{q}) \in \mathcal{K}_{i j}$ found by reciprocal matching%
in feature space, and measures what fraction of them is "correct", in the sense that they lie within a threshold $\tau_1\!=\!10\,$cm after registering the two scans with the ground truth transformation $\overbar{T}_\mathbf{P}^ \mathbf{Q}$:
\begin{equation}
\mathrm{IR} = \frac{1}{\left|\mathcal{K}_{ij}\right|} \sum_{\left(\mathbf{p}, \mathbf{q}\right) \in \mathcal{K}_{ij}} \big[ ||\overbar{\mathbf{T}}_\mathbf{P}^ \mathbf{Q}(\mathbf{p})-\mathbf{q}||_2<\tau_{1} \big]  \;,
\end{equation}
with $[\cdot]$ the Iverson bracket.

\textbf{Feature Match recall} (FMR)~\cite{deng2018ppfnet} measures the fraction of point cloud pairs for which, based on the number of inlier correspondences, it is \emph{likely} that accurate transformation parameters can be recovered with a robust estimator such as RANSAC.
Note that FMR only checks whether the inlier ratio is above a threshold $\tau_2=0.05$. It does not test if the transformation can actually be determined from those correspondences, which in practice is not always the case, since their geometric configuration may be (nearly) degenerate, e.g., they might lie very close together or along a straight edge.
A single pair of point clouds counts as suitable for registration if%
\begin{equation}
    IR > \tau_2
\end{equation}

\textbf{Registration recall}~\cite{choi2015robust} is the most reliable metric, as it measures end-to-end performance on the actual task of point cloud registration. Specifically, it looks at the set of ground truth correspondences $\mathcal{H}_{i j}^{*}$ after applying the estimated transformation $T_\mathbf{P}^ \mathbf{Q}$, computes their root mean square error,
\begin{equation}
    \mathrm{RMSE} = \sqrt{\frac{1}{\left|\mathcal{H}_{i j}^{*}\right|} \sum_{\left(\mathbf{p}, \mathbf{q}\right) \in \mathcal{H}_{i j}^{*}}||\mathbf{T}_\mathbf{P}^ \mathbf{Q}(\mathbf{p}) -\mathbf{q}||_2^2}\;,
\end{equation}
and checks for what fraction of all point pairs $\mathrm{RMSE}\!<\!0.2$.
In keeping with the original evaluation script of \emph{3DMatch}, immediately adjacent point clouds are excluded, since they have very high overlap by construction.



\textbf{Chamfer distance} measures the quality of registration on synthetic data. We follow ~\cite{yew2020rpm} and use the \emph{modified} Chamfer distance metric:
\begin{equation}
\begin{aligned}
\tilde{C D}(\mathbf{P}, \mathbf{Q}) = & \frac{1}{|\mathbf{P}|} \sum\limits_{\mathbf{p} \in \mathbf{P}} \min\limits_{\mathbf{q} \in \mathbf{Q}_{\text{raw}}}\|\mathbf{T}_\mathbf{P}^ \mathbf{Q}(\mathbf{p})-\mathbf{q}\|_{2}^{2} + \\
& \frac{1}{|\mathbf{Q}|} \sum\limits_{\mathbf{q} \in \mathbf{Q}} \min\limits_{\mathbf{p} \in \mathbf{P}_{\text{raw}}}\|\mathbf{q}- \mathbf{T}_\mathbf{P}^ \mathbf{Q}(\mathbf{p})\|_{2}^{2}
\end{aligned}
\end{equation}
where $\mathbf{P}_{\text{raw}}\in \mathbb{R}^{2048\times3}$ and $\mathbf{Q}_{\text{raw}}\in \mathbb{R}^{2048\times3}$ are \emph{raw} source and target point clouds, $\mathbf{P} \in \mathbb{R}^{717\times3}$ and $\mathbf{Q} \in \mathbb{R}^{717\times3}$ are \emph{input} source and target point clouds. 

\textbf{Relative translation and rotation errors} (RTE/RRE) measures the deviations from the ground truth pose as: 
\begin{equation}
\begin{aligned}
\text{RTE} &= \|\mathbf{t} - \overbar{\mathbf{t}}\|_2\\
\text{RRE} &=\arccos\big(\frac{\mathrm{trace}{(\mathbf{R}^{T}\overbar{\mathbf{R}})-1}}{2}\big)
\end{aligned}
\end{equation}
where $\mathbf{R}$ and $\mathbf{t}$ denote the estimated rotation matrix and translation vector, respectively.

\textbf{Empirical Cumulative Distribution Function} (ECDF) measures the distribution of a set of values:
\begin{equation}
\begin{aligned}
\text{ECDF} (x) = \frac{\big|\{o_i < x\}\big|}{\big|O\big|}
\end{aligned}
\end{equation}
where $O$ is a set of values(ovelap ratios in our case) and $x \in [\min\{O\}, \max\{O\}]$.

\subsection{Dataset preprocessing}
\label{sec:datasets_supp}
\paragraph{3DMatch}
\cite{zeng20163dmatch} is a collection of 62 scenes, combining earlier data from Analysis-by-Synthesis~\cite{valentin2016learning}, 7Scenes~\cite{shotton2013scene}, SUN3D~\cite{xiao2013sun3d}, RGB-D Scenes v.2~\cite{lai2014unsupervised}, and Halber~\etal~\cite{Halber2016StructuredGR}.  The official benchmark splits the data into 54 scenes for training and 8 for testing. Individual scenes are not only captured in different indoor spaces (e.g., bedrooms, offices, living rooms, restrooms) but also with different depth sensors (e.g., Microsoft Kinect, Structure Sensor, Asus Xtion Pro Live, and Intel RealSense). \emph{3DMatch} provides great diversity and allows our model to generalize across different indoor spaces. Individual scenes of \emph{3DMatch} are split into point cloud fragments, which are generated by fusing 50 consecutive depth frames using TSDF volumetric fusion~\cite{curless1996volumetric}. As a preprocessing step, we apply voxel-grid downsampling to all point clouds, and if multiple points fall into the same voxel, we randomly pick one.

\paragraph{ModelNet40}
For each CAD model of \emph{ModelNet40}, 2048 points are first generated by uniform sampling and scaled to fit into a unit sphere. Then we follow~\cite{yew2020rpm} to produce partial scans: for source partial point cloud, we uniformly sample a plane through the origin that splits the unit sphere into two half-spaces, shift that plane along its normal until $\lfloor 2048\cdot p_v \rfloor$ points are on one side, and discard the points on the other side; the target point cloud is generated in the same manner; then the two resulting, partial point clouds are randomly rotated, translated and jittered with Gaussian noise. For the rotation, we sample a random axis and a random angle \textless45$^\circ$. The translation is sampled in the range $[-0.5,0.5]$. Gaussian noise is applied per coordinate with $\sigma\!=\!0.05$. Finally, 717 points are randomly sampled from the $\lfloor 2048\cdot p_v \rfloor$ points.


\paragraph{odometryKITTI}
The dataset was captured using a Velodyne HDL-64 3D laser scanner by driving around the mid-size city of Karlsruhe, in rural areas and on highways. The ground truth poses are provided by GPS/IMU system. We follow ~\cite{bai2020d3feat} to use ICP to reduce the noise in the ground truth poses. 


\subsection{Implementation and training}
\input{\subdir/tables/hyperparameters}
\label{sec:training_supp}
For 3DMatch/Modelnet/KITTI, we train \acro\ using Stochastic Gradient Descent for $30$/ $200$/ $150$ epochs, with initial learning rate $0.005$/ $0.01$/ $0.05$, momentum $0.98$, and weight decay $10^{-6}$. The learning rate is exponentially decayed by 0.05 after each epoch. Due to memory constraints we use batch size $1$ in all experiments. The dataset-dependent hyper-parameters which include number of negative pairs in circle loss $n_p$, temperature factor $\gamma$, voxel size $V$, search radius for positive pair $r_p$, safe radius $r_s$, overlap and matchability radius $r_o$ and $r_m$ are given in Tab.~\ref{tab:hyperparameters}. On odometryKITTI dataset, we take the curriculum learning~\cite{bengio2009curriculum} strategy to gradually learn sharper local descriptors by adjusting $n_p$. For more details please see our code.

\subsection{Network architecture}
\label{sec:network_arch_supp}
\input{\subdir/tables/compare_network}
The detailed network architecture of \acro\ is depicted in Fig.~\ref{fig:network_arch_detailed}. Our model is built on the KPConv implementation from the D3Feat repository.%
\footnote{\url{https://github.com/XuyangBai/D3Feat.pytorch}} %
We complement each KPConv layer with instance normalisation Leaky ReLU activations. The $l$-th strided convolution is applied to a point cloud dowsampled with voxel size $2^{l}\cdot V$. Upsampling in the decoder is performed by querying the associated feature of the closest point from the previous layer.  

With $\approx$20k points after voxel-grid downsampling, the point clouds in \emph{3DMatch}  are much denser than those of \emph{ModelNet40} with only 717 points. Moreover, they also have larger spatial extent with bounding boxes up to $3\times3\times3$~$\text{m}^3$, while \emph{ModelNet40} point clouds are normalised to fit into a unit sphere.
To account for these large differences, we slightly adapt the encoder and decoder per dataset, but keep the same overlap attention model. Differences in network hyper-parameters are shown in Tab.~\ref{tab:compare_network}. 


\input{\subdir/tables/detailed_3dmatch}
\subsection{Additional results}
\label{sec:additional_results_supp}
\paragraph{Detailed registration results}
We report detailed per-scene \textit{Registration Recall (RR)}, \textit{Relative Rotation Error (RRE)} and \textit{Relative Translation Error (RTE)} in Tab.~\ref{tab:detailed_3dmatch}. RRE and RTE are only averaged over successfully registered pairs for each scene, such that the numbers are mot dominated by gross errors from complete registration failures. We get the highest RR and lowest or second lowest RTE and RRE for almost all scenes, this further shows that our overlap attention module together with probabilistic sampling supports not only robust, but also accurate registration.  


\paragraph{Feature match recall}
Finally, Fig.~\ref{fig:fmr} shows that our descriptors are robust and perform well over a wide range of thresholds for the allowable inlier distance and the minimum inlier ratio. Notably, \acro\ consistently outperforms D3Feat that uses a similar KPConv backbone.


\subsection{Additional ablation studies}
\label{sec:addtional_ablation_supp}

\input{\subdir/tables/ablate_matchability}
\paragraph{Ablations of matchability score}
We find that probabilistic sampling guided by the product of the overlap and matchability scores attains the highest RR. Here we further analyse the impact of each individual component. We first construct a baseline which applies random sampling (\textit{rand}) over conditioned features, then we sample points with probability proportional to overlap scores (\textit{prob. (o)}), to matchability scores (\textit{prob. (m)}), and to the combination of the two scores (\textit{prob. (om)}). As shown in Tab.~\ref{tab:ablate_matchability}, \textit{rand} fares clearly worse, in all metrics. Compared to \textit{prob. (om)}, either \textit{prob. (o)} or \textit{prob. (m)} can achieve comparable results on \emph{3DMatch}; the performance gap becomes big on the more challenging \emph{3DLoMatch} dataset, where our \textit{prob. (om)} is around 4 pp better in terms of RR. 

\input{\subdir/tables/FCGF_OA}
\paragraph{Ablations of overlap attention module with FCGF}
To demonstrate the flexibility of our model, we additionally add proposed overlap attention module to FCGF model. We train it on \emph{3DMatch} dataset with our proposed loss for 100 epochs, the results are shown in Tab.~\ref{tab:ablate_fcgf}. It shows that FCGF can also greatly beneï¬t from the overlap attention module. Registration recall almost doubles when sampling only 250 points on the challenging \emph{3DLoMatch} benchmark. 

\input{\subdir/figures/tex/FMR}
\subsection{Timings}
\label{sec:timing}
\input{\subdir/tables/timing}
We compare the runtime of \acro\ with FCGF%
\footnote{All experiments were done with MinkowskiEngine v0.4.2.} %
~\cite{Choy2019FCGF} and D3Feat%
\footnote{We use its PyTorch implementation.} %
~\cite{bai2020d3feat} on \emph{3DMatch}. For all three methods we set voxel size $V\!=\!2.5\,$cm and batch size 1. The test is run on a single GeForce GTX 1080 Ti with Intel(R) Core(TM) i7-7700K CPU @ 4.20GHz, 32GB RAM. The most time-consuming step of our model, and also of D3Feat, is the data loader, as we have to pre-compute the neighborhood indices before the forward pass. With its smaller encoder and decoder, but the additional overlap attention module, \acro\ is still marginally faster than D3Feat. FCGF has a more efficient data loader that relies on sparse convolution and queries neighbors during the forward pass. See Tab.~\ref{tab:3DMatch_times}.

\subsection{Qualitative visualization}
\label{sec:qualitative_supp}
We show more qualitative results in Fig.~\ref{fig:3dmatch_supp} and Fig.~\ref{fig:modelnet_supp} for \emph{3DLoMatch} and \emph{ModelLoNet} respectively. The input points clouds are rotated and translated here for better visualization of overlap and matchability scores. 

\input{\subdir/figures/tex/detailed_network}
\input{\subdir/figures/tex/3dmatch_supp}
\input{\subdir/figures/tex/supp_modelnet}



