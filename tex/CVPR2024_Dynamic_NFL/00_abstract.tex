\section*{Abstract}
We introduce \dynfl, a novel neural field-based approach for high-fidelity re-simulation of LiDAR scans in dynamic driving scenes. \dynfl processes LiDAR measurements from dynamic environments, accompanied by bounding boxes of moving objects, to construct an editable neural field. This field, comprising separately reconstructed static backgrounds and dynamic objects, allows users to modify viewpoints, adjust object positions, and seamlessly add or remove objects in the re-simulated scene.
%
% Existing LiDAR simulation methods using real-world measurements typically struggle with accurately modeling LiDAR physical properties or are limited to static scenes. In contrast, \dynfl employs a neural Signed Distance Function (SDF) representation to improve geometric fidelity and ensures physical accuracy using a two-way transmittance model, building upon the principles established in NFL. 
A key innovation of our method is the neural field composition technique, which effectively integrates reconstructed neural assets from various scenes through a ray drop test, accounting for occlusions and transparent surfaces.
Our evaluation with both synthetic and real-world environments demonstrates that \dynfl substantial improves dynamic scene simulation based on LiDAR scans, offering a combination of physical fidelity and flexible editing capabilities. \url{}