\section{Introduction}
We introduce a neural representation for the purpose of reconstructing and manipulating LiDAR scans of dynamic driving scenes. 
Counterfactual re-simulation is an emerging application in the realm of autonomous driving, offering a unique approach to examining "what if" scenarios. This method involves creating a reconstruction of a real-world event, termed as \textit{digital twin} and then applying various modifications to it. These could include altering the environmental conditions, changing the action of some agent, or introducing additional scene elements. Analyzing the outcomes of these edited scenarios provides insights into the functioning of the perception system, moreover they can be used to obtain training data for rare situations.

The essence of counterfactual re-simulation is the capability to authentically recreate variations of the original, factual observation. We address this challenge in the context of LiDAR on autonomous vehicles (AV). Existing approaches to LiDAR re-simulation have important limitations. Conventional simulators such as CARLA~\cite{dosovitskiy2017carla} and NVIDIA DRIVE Sim are capable of modeling LiDAR sensors. However, their reliance on manually designed 3D simulation assets requires significant human effort. LiDARsim~\cite{manivasagam2020lidarsim} aims to remedy this by reconstructing vehicles and scenes from real measurements. While producing encouraging results, its two-stage LiDAR modeling lacks realism, particularly in terms of physical effects like multi-returns and reflected intensity, which were shown 
 to matter for downstream processing~\cite{guillard2022learning}. Following NeRF's~\cite{mildenhall2020nerf} success in camera view synthesis, some works have applied neural fields for LiDAR modeling~\cite{Huang2023nfl, tao2023lidar, zhang2023nerf}. In particular, Neural LiDAR Fields (NFL)\cite{Huang2023nfl} developed a physically inspired LiDAR volumetric rendering scheme that accounts for two-way transmittance and beam width, allowing faithful recovery of secondary returns, intensity, and ray drops. These models are, however, limited to static scenes that do not change while multiple input views are scanned, and are thus of limited use for re-simulation in the presence of moving traffic. Recently, UniSim~\cite{yang2023unisim} followed Neural Scene Graph~\cite{Ost_2021_CVPR} in modeling road scenes as sets of movable NeRF instances on top of a static background. UniSim introduced a unified synthesis approach for camera and LiDAR sensors, but ignored physical sensor properties like two-way transmittance and beam width~\cite{Huang2023nfl}.

We present \dynfl, a novel approach for re-simulating LiDAR views of driving scenarios. Our method builds upon a neural SDF that enables an accurate representation of scene geometry, while at the same time enforcing physical accuracy by modeling two-way transmittance, like NFL~\cite{Huang2023nfl}. 
%
Our primary contribution is a method for compositing neural fields that accurately integrates LiDAR measurements from individual fields representing different scene assets. With the help of a ray drop test, we effectively manage occlusions and transparent surfaces. This not only ensures physical accuracy, but also facilitates the inclusion of assets reconstructed from a variety of static and dynamic scenes, thereby enhancing control over the simulated content. Our method bridges the gap between the physical fidelity of the re-simulation and flexible editing of dynamic scenes.
%
We validate \dynfl with both synthetic and real-world data, focusing on three key areas: \textit{(i)} high-quality view synthesis, \textit{(ii)} perceptual fidelity, and \textit{(iii)} asset manipulation. We find that our approach outperforms baseline models \wrt both range and intensity. Its synthetic outputs also show higher agreement with real scans in terms of object detection and segmentation. Furthermore, \dynfl enables not only removal, duplication and repositioning of assets within the same scene, but also the inclusion of assets reconstructed in other scenes, paving the way for new applications.



