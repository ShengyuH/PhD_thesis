\section{Related work}
\subsection{Neural radiance fields and volume rendering} % {Neural Radiance Fields for Novel View Synthesis}


Neural Radiance Fields (NeRF)~\cite{mildenhall2020nerf} have demonstrated remarkable success in novel-view image synthesis through neural volume rendering. These fields are characterized by the weights of Multilayer Perceptrons (MLPs), which enable the retrieval of volume density and RGB colors at any specified point within the field for image compositing via volume rendering. Several studies~\cite{barron2021mip,barron2022mip,verbin2022ref,chen2022tensorf,fridovich2022plenoxels} have subsequently advanced NeRF's rendering quality by addressing challenges such as reducing aliasing artifacts~\cite{barron2021mip}, scaling to unbound large-scale scenarios~\cite{barron2022mip}, and capturing specular reflections on glossy surfaces~\cite{verbin2022ref}.
Certain works~\cite{chen2022tensorf,fridovich2022plenoxels,mueller2022instant,kerbl20233d} have explored more effective representations of radiance fields. TensorsRF~\cite{chen2022tensorf} employs multiple compact low-rank tensor components, such as vectors and matrices, to represent the radiance field. Plenoxels~\cite{fridovich2022plenoxels} accelerates NeRF training by replacing MLPs with explicit plenoptic elements stored in sparse voxels and factorizing appearance through spherical-harmonic functions.
M\"uller et al.~\cite{mueller2022instant} achieved a substantial acceleration in rendering speed by employing a representation that combines trainable multi-resolution hash encodings (MHE) with shared shallow MLP networks. Kerbel et al.~\cite{kerbl20233d} introduce a novel volume rendering method utilizing 3D Gaussians to represent the radiance field and rendering images based on visibility-aware splatting of 3D Gaussians.


% Neural Radiance Fields (NeRF)~\cite{mildenhall2020nerf} achieved great success in utilizing neural volume rendering for novel-view image synthesis. NeRF represents the 3D radiance fields with the weights in multiple-layer perceptrons (MLPs), which take input of arbitrary 3-DoF position on a ray and the ray's 2-DoF view direction, and output volume density and RGB colors for compositing images via volume rendering. Many follow-ups of NeRF have emerged in recent years, focusing on improving the speed and quality of the NeRF. For example, Mip-NeRF~\cite{barron2021mip} performing volume rendering with conical frustums instead of rays can reduce the aliasing artifacts for the rendered image. Mip-Nerf360~\cite{barron2022mip} extends Mip-NeRF with a distortion-based regularizer that can further strengthen the anti-aliasing capability in unbound large-scale scenarios.
% RefNerf~\cite{verbin2022ref} replaces NeRF's view-dependent outgoing radiance with reflected radiance for better capturing the specular reflections on glossy surfaces. TensorsRF~\cite{chen2022tensorf} represents the radiance field with multiple compact low-rank tensor components like vectors and matrices. The effective and compact presentation leads to fast convergence and high-quality rendering.
% Plenoxels~\cite{fridovich2022plenoxels} speeds up the training speed of NeRF by replacing the MLPs in NeRF with explicit plenoptic elements saved in sparse voxels and factorizing the appearance via spherical-harmonic functions. 
% M\"uller et al.~\cite{mueller2022instant} propose representing the radiance field by a shared shallow MLP network and trainable multi-resolution hash encodings (MHE). The MHE allows efficient parallelized training on modern GPUS, achieving significant speeding up on neural volume rendering. Kerbel et al.~\cite{kerbl20233d} propose a novel volume rendering method that utilizes 3D Gaussians to represent the radiance field, and renders images based on visibility-aware splatting of 3D Gaussians.

\subsection{Dynamic neural radiance fields} 
Neural fields \cite{xie2022neural} can be extended to represent dynamic scenes. On top of the \textit{canonical} scene representation, some methods~\cite{pumarola2020d, park2021nerfies, park2021hypernerf,yuan2021star} additionally model the 4D deformation fields. Meanwhile, some other works learn a space-time correlated~\cite{kplanes_2023, li2020neural, attal2023hyperreel, liu2023robust}, or decomposed~\cite{turki2023suds,wu2022d,yang2023emernerf} neural field to encode the 4D scenes, achieving fine-grained reconstruction of the geometry and the appearance.
%
Some other methods decompose the scene into static and dynamic parts, and model each dynamic actor with dedicated neural fields. 
Neural Scene Graph~\cite{Ost_2021_CVPR} and Panoptic Neural Fields~\cite{KunduCVPR2022PNF} treat every dynamic object in the scene as a node, and synthesize photo-realistic RGB images by jointly rendering from both dynamic nodes and static background. UniSim\cite{yang2023unisim} employs neural SDF representation to model dynamic scenes in driving scenarios, and render in a similar way to Neural Scene Graph~\cite{Ost_2021_CVPR}.
% \stefan{``Star: Self-supervised tracking and reconstruction of rigid objects in motion with neural rendering'' this is another one that comes to mind regarding composed dynamic NeRFs.}

\subsection{Neural surface representation}
A fundamental challenge for NeRF and its variants involves accurately recovering the underlying 3D surface from the implicit radiance field. Surfaces obtained by thresholding on the volume density of NeRF often exhibit noise~\cite{wang2021neus, yariv2021volume}. To address this, implicit surface representations like Occupancy~\cite{niemeyer2020differentiable, oechsle2021unisurf} and signed distance functions (SDF)~\cite{wang2021neus, yariv2021volume, yu2022monosdf, sun2022neural, wang2022hf, zuo2023incremental, li2023neuralangelo, wang2023neus2} in grid maps are commonly integrated into neural volume rendering techniques.

NeuS~\cite{wang2021neus} introduces a neural SDF representation for surface reconstruction, proposing an unbiased weight function for the appearance composition process in volume rendering. Similarly, VolSDF~\cite{yariv2021volume} models scenes with a neural SDF and incorporates the SDF into the volume rendering process, advocating a sampling strategy of the viewing ray to bound opacity approximation error. Neuralangelo~\cite{li2023neuralangelo} improves surface reconstruction using the multi-resolution hash encoding (MHE)~\cite{mueller2022instant} and SDF-based volume rendering~\cite{wang2021neus}. While these methods might deliver satisfying dense surface reconstructions, their training is time-consuming, taking hours for a single scene.
Voxurf~\cite{wu2022voxurf} offers a faster surface reconstruction method through a two-stage training procedure, recovering the coarse shape first and refining details later. Wang et al.~\cite{wang2023neus2} expedites NeuS training to several minutes by predicting SDFs through a pipeline composed of MHE and shallow MLPs.

Many works also incorporate distances measured by LiDAR as auxiliary information to constrain the radiance field. For instance, works~\cite{chang2023neural, wang2023neural} render depth by accumulating volume density and minimizing depth discrepancies between LiDAR and render depth during training. Rematas et al.~\cite{rematas2022urban} enforces empty space between the actual surface and the ray origin.




% A fundamental problem of NeRF and its variants is to recover the underlying 3D surface from the radiance field accurately.
% Surfaces recovered by thresholding on the volume density of NeRF are often noisy~\cite{wang2021neus, yariv2021volume}. Occupancy~\cite{niemeyer2020differentiable, oechsle2021unisurf} and signed distance function (SDF)~\cite{wang2021neus, yariv2021volume, yu2022monosdf, sun2022neural, zuo2023incremental, li2023neuralangelo, wang2023neus2} grid maps are commonly used for representing surface implicitly, and they are also incorporated into the neural volume rendering in many works.
% %
% NeuS~\cite{wang2021neus} presents a neural SDF representation for surface reconstruction and proposes an unbiased weight function for the appearance composition process in volume rendering. 
% %
% Similarly, VolSDF~\cite{yariv2021volume} also models the scene with a neural SDF and gets the SDF involved in the volume rendering process. It advocates a sampling strategy of the viewing ray that facilitates to bound the opacity approximation error.  
% %
% HF-NeuS~\cite{wang2022hf} decomposes neural SDF presentation into a base function and a displacement function with a coarse-to-fine strategy to increase the high-frequency details gradually.
% %
% NeuralLangelo~\cite{li2023neuralangelo} can deliver satisfying surface reconstruction built on multi-resolution hash encoding (MHE)~\cite{mueller2022instant}, and SDF-based volume rendering~\cite{wang2021neus}.
% % It computes numerical gradients beyond the resolution of the local cell for smoothing the optimization on SDF, and optimizes the multi-level hashing encoding progressively to recover coarse-to-fine geometric details.
% %
% Although Neus and NeuralLangelo can perform satisfying dense reconstruction of the surfaces,  both NeuS and NerualLangelo are time-consuming -- taking hours to train for a single scene.
% %
% Voxurf~\cite{wu2022voxurf} is a faster surface reconstruction method following a two-stage training procedure that recovers the coarse shape first and refines details later.
% %
% Wang et al.~\cite{wang2023neus2} expedite the training of NeuS to several minutes by predicting SDFs by a pipeline composited by MHE and shallow MLPs. 
% % They also present a simple calculation of the second-order derivatives tailored for efficient paralleled computation on GPU.

% Distances measured by LiDAR are also used in many works as auxiliary information for constraining the radiance field. The works~\cite{chang2023neural, wang2023neural} render depth by accumulating the volume density and minimizing the depth discrepancy between LiDAR depth and render depth during training. Besides the depth constraint from LiDAR measurements, the work~\cite{rematas2022urban} also enforces the empty between the hit surface and the ray origin.






% Neural volume can also represent dynamic scenes. One track of works learns to deform the static NeRF \cite{pumarola2020d, park2021nerfies, park2021hypernerf} or learns a space-time correlated neural field to encode the 4D scenes \cite{kplanes_2023, li2020neural, attal2023hyperreel, liu2023robust} to achieve fine reconstruction details of the geometry. The other track of modeling dynamic scenes using NeRF is dedicated for large scenes like driving scenarios, where they model each dynamic actor as well as the background and composite them together, Neural Scene Graph\cite{Ost_2021_CVPR} and Panoptic Neural Fields\cite{KunduCVPR2022PNF} treat every dynamic object in the scene as a node, and jointly render RGB values based on the queried density in each dynamic nodes and produce photo-realistic images. UniSim\cite{yang2023unisim} proposes a novel multi-sensor fused simulator using SDF representation to model dynamic scenes in driving scenes. These methods are equipped with scene editing functionality hence augment the dataset for training self driving vehicles(SDV). 



\subsection{LiDAR simulation} 
While simulators like CARLA~\cite{dosovitskiy2017carla} and AirSim~\cite{shah2018airsim} can simulate LiDAR data, they suffer from expensive human annotation requirements and a notable sim-to-real gap due to limited rendering quality. Generative model-based methods for LiDAR synthesis~\cite{caccia2019deep,zyrianov2022learning} offer an alternative but often lack control and produce distorted geometries~\cite{li2023pcgen}.
Learning-based approaches~\cite{li2023pcgen,fang2020augmented,manivasagam2020lidarsim} try to enhance realism by transferring real scan properties to simulations. For example, \cite{guillard2022learning} uses a RINet trained on RGB and real LiDAR data to augment simulated scan qualities. LiDARsim~\cite{manivasagam2020lidarsim} employs ray-surfel casting with explicit disk surfels for more accurate simulations.
Huang et al.~\cite{Huang2023nfl} proposed Neural LiDAR Fields (NFL), combining neural fields with a physical LiDAR model for high-quality synthesis, although it's limited to static scenes and can produce noisy outputs due to its unconstrained volume density representation.
UniSim~\cite{yang2023unisim} constructs neural scene representations from realistic LiDAR and camera data, using SDF-based volume rendering for sensor measurement generation at novel viewpoints.
% Our approach surpasses UniSim's performance, attributed to the physically-accurate SDF-based volume rendering for active sensors, and our proposed scene composition method that effectively integrates static and dynamic neural fields.


% There are dedicated robotics simulators~\cite{dosovitskiy2017carla,shah2018airsim,yue2018lidar} that simulate LiDAR measurements. However, both the sensory measuring model and the synthetic scenarios are filled with handcrafted 3D assets which inevitably incurs a large sim-to-real gap. Some other methods~\cite{caccia2019deep,zyrianov2022learning} rely on generative models for creating synthesized LiDAR scans. However, this stream of works lacks controllability and might suffer from generalization issues during test time~\cite{manivasagam2023towards}. 
% %
% For generating realistic LiDAR data, some other methods~\cite{attal2021torf,manivasagam2020lidarsim,li2023pcgen, Huang2023nfl, yang2023unisim} first reconstruct a model of the 3D scene from real LiDAR data, and then generate synthetic LiDAR scans from the reconstructed model.
% To this end, LiDARsim~\cite{manivasagam2020lidarsim} creates a model of the scene from real LiDAR data and enables more realistic LiDAR simulation. The scene is resented by explicit disk surfels. For modelling dynamic vehicles, it aggregates the LiDAR points sampled on individual vehicles indicated by annotated bounding boxes. Ray casting through the surferls is performed for creating sythetic LiDAR scans. 
% % At test time, it synthesizes vehicles by ray casting through the surfels derived from aggregated points, and achieves decent quality due to the imperfect discrete surfel representation of the scene, while fast synthesis speed can be achieved attributed to the high efficiency of ray casting for surfels.
% %
% Huang et al.~\cite{Huang2023nfl} develop a novel Neural LiDAR simulator by representing the scene with a radiance field with volume density embedded, and have displayed promising simulation results on many datasets. However, the volume density formulation is insufficient for modeling physical surfaces with high fidelity~\cite{wang2021neus,yariv2021volume,li2023neuralangelo}. The reconstructed surface on the planar regions is not smooth and contaminated by tiny particle artifacts sometimes, while the reconstructed surfaces on complex objects like trees and bushes could exhibit more chaotic artifacts.
% %
% The closet work to ours is Unisim~\cite{yang2023unisim}, which takes in a
% a sequence of LiDAR and camera data for constructing neural scene presentations for both static background and dynamic objects, and creates new sensor observations by neural rendering from new viewpoints. Our method exhibits better performance than UniSim due to our dedicated SDF-based volume rendering formulation for active sensor and neural fields composition techniques. 
