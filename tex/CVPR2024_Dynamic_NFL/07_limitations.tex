\section{Limitations and future work}
We present DyNFL, a compositional neural fields approach for LiDAR re-simulation. Our method excels previous art in both static and dynamic scenes, offering powerful scene editing capabilities that open up opportunities for generating diverse and high-quality scenes, to evaluate an autonomy system trained only on real data in closed-loop.

Despite achieving the state-of-the-art performance, there are still limitations we aim to address in future work. Firstly, \dynfl faces challenges in view synthesis of dynamic vehicles from unseen angles. This difficulty arises from the complexity of creating an a-priori model that can accurately complete unseen regions and simulate point cloud noise, ray drops patterns etc. Secondly, our method currently relies on object detection and tracking annotations, and its performance may be compromised when given inaccurate labels. Overcoming this dependency, exploring 4D representations while retaining scene editing flexibility, stands out as a crucial challenge for future research.


% We present DyNFL, a dynamic neural LiDAR representation method for new view synthesis. Our method excels the SOTA methods in both static and dynamic scenes, offering powerful scene editing capabilities that open up opportunities for generating diverse training scenarios for LiDAR perception models in self-driving vehicles at a minimal cost.

% Despite achieving state-of-the-art performance, there are still limitations we aim to address in future work. Firstly, our LiDAR models for dynamic vehicles face challenges in synthesis from unseen angles. This difficulty arises from the complexity of creating an a-priori model that accurately simulates point cloud noise, ray drops due to mirror reflections, attenuation, and multi-returns on the object boundary. Secondly, our method currently relies on dynamic annotations for training, and its performance may be compromised when encountering inaccurate annotations. Overcoming this dependency, exploring 4D representations while retaining scene editing flexibility, stands out as a crucial challenge for future research.