\section{Neural rendering of the dynamic scene}
In this section, we present the methodology for rendering LiDAR scans from the neural scene representation. We begin by revisiting the density-based volume rendering formulation for active sensors~\cite{Huang2023nfl} in \cref{sec:vol_render_background}. Subsequently, we explore the extension of this formulation to SDF-based neural scene representation in \cref{sec:sdf_vol_render}. Finally, we provide a detailed discussion on rendering LiDAR measurements from individual neural fields in~\cref{sec:dynamic_nfl_rendering} and the process of composing results from different neural fields in \cref{sec:neural_fields_composition}.



\subsection{Volume rendering for active sensor} \label{sec:vol_render_background}
LiDAR utilizes laser beam pulses to determine the distance to the nearest reflective surface by analyzing full waveform profile of the returned radiant power. The radiant power $P(\zeta)$ from range $\zeta$ is the result of a convolution between the pulse power $P_e(t)$ and the impulse response $H(\zeta)$, defined as~\cite{hahner2021fog,hahner2022lidar,Huang2023nfl}:
\begin{equation}
    P(\zeta) = \int_0^{2\zeta/c} P_e(t) H(\zeta - \frac{ct}{2}) \; dt\;.
\label{eq:lidar}
\end{equation}
The impulse response $H(\zeta)$ is a product of the target and sensor impulse responses: $H(\zeta) = H_T(\zeta)\cdot H_S(\zeta)$, and the individual components are expressed as:
\begin{equation}
    H_T(\zeta) = \frac{\reflectance}{\pi} \cos(\theta) \delta(\zeta - \bar{\zeta})\;, \quad  H_s(\zeta) = T^2(\zeta) \frac{A_e}{\zeta^2}\;,
\label{eq:ht}
\end{equation}
where $\reflectance$ represents the surface reflectance, $\theta$ denotes incidence angle, $\bar{\zeta}$ is the ground truth distance to the nearest reflective surface, $T(\zeta)$ and $A_e$ describe the transmittance at range $\zeta$ and sensor's effective area, respectively. Due to the non-differentiability introduced by the indicator function $\delta(\zeta - \bar{\zeta})$, ~\cref{eq:lidar} is non-differentiable and is thus not suitable for solving the inverse problem. NFL~\cite{Huang2023nfl} solves it by extending it into a probabilistic formulation given by:
\begin{equation}
P(\zeta) = C \cdot \frac{T^2(\zeta) \cdot \density_\zeta  \reflectance_\zeta}{\zeta^2} \cos(\theta)\;.
\label{eq:radiance}
\end{equation}
Here, $C$ accounts for the constant values, and $\sigma_\zeta$ represents the density at range $\zeta$. The radiant can be reconstructed using the volume rendering formulation:
\begin{equation}
      P
      =\!\sum_{j=1}^N \int_{\zeta_j}^{\zeta_{j+1}}\!\!C \frac{T^2({\zeta}) \cdot \density_\zeta \reflectance_\zeta}{\zeta^2} \cos(\theta_j) \; d\zeta
      =\!\sum_{j=1}^N w_j \reflectance_{\zeta_j}',
\label{eq:radiant_inter}
\end{equation}
where the weights $w_j = 2 \opacity_{\zeta_j} \cdot\prod_{i=1}^{j-1}(1 - 2 \opacity_{\zeta_i}).$
Here $\alpha_{\zeta_j}$ is the discrete opacity at range $\zeta_j$. Please refer to~\cite{Huang2023nfl} for more details.


\subsection{SDF-based volume rendering for active sensor} \label{sec:sdf_vol_render}
% \orl{do we use sdf for both static and dynamic parts?} \hanfeng{yes}
A neural scene representation based on probabilistic density often results in surfaces with noticeable noise due to insufficient surface regularization~\cite{wang2021neus}. To address this, we opt for a signed distance-based scene representation and establish the volume rendering formulation within the framework of an active sensor. Building upon SDF-based volume rendering for passive sensors~\cite{wang2021neus}, we compute the opaque density $\tilde{\density}_{\zeta_i}$ as follows:
\begin{equation}
\tilde{\density}_{\zeta_i} = \max\left(\frac{-\frac{{\textrm{d}}\Phi_s}{{\textrm{d}} \zeta_i}(f(\zeta_i))}{\Phi_s(f(\zeta_i))},0\right),
\label{eq:sigmoid_density}
\end{equation}
where $\Phi_s(\cdot)$ represents the Sigmoid function, $f(\zeta)$ evaluates the signed distance to the surface at range $\zeta$ along the ray $\ray$. 

Next, we substitute the density $\density$ in \cref{eq:radiant_inter} with opaque density from \cref{eq:sigmoid_density} and re-evaluate the radiant power and weights as:
\begin{equation}
      P
      =\!\sum_{j=1}^N \trans^2_{\zeta_j} \tilde{\alpha}_{\zeta_j} \reflectance_{\zeta_j}',\quad \tilde{w}_j = 2 \tilde{\opacity}_{\zeta_j} \cdot\prod_{i=1}^{j-1}(1 - 2 \tilde{\opacity}_{\zeta_i})\;.
\end{equation}
In this context, $\tilde{\alpha}_{\zeta_j}$ is computed as:
\begin{equation}
    \tilde{\alpha}_{\zeta_j} = \max\left(\!\frac{{\Phi_s(f(\zeta_j))}^2 -{\Phi_s(f(\zeta_{j+1}))}^2}{{2\Phi_s(f(\zeta_j))}^2},0\right).
    \label{eq:new_weights}
\end{equation}
Please refer to the appendix for more details.


\subsection{Volume rendering for LiDAR measurements}\label{sec:dynamic_nfl_rendering}
Consider rendering the LiDAR measurements from a single neural field, we employ the hierarchical sampling\cite{wang2021neus} technique to sample a total of $N_s= N_u + N_i$ points along each ray, where $N_u$ points are uniformly sampled, and $N_i$ points are probabilistically sampled based on the weights along the ray, facilitating denser sampling in proximity to the surface. Subsequently, we compute the weights for the $N_s$ points following~\cref{eq:new_weights}. The rendering of range, intensity, and ray drop for each ray can be expressed through volume rendering as follows: $y_\text{est} = \sum_{j=1}^{N_s} w_j y_j$, where $y \in \{\zeta, \intensity, \pdrop\}$.


\subsection{Neural rendering for multiple fields}\label{sec:neural_fields_composition}
Our full neural scene representation comprises $N+1$ neural fields as discussed in Sec.~\ref{sec: decomposition}. Rendering from all these fields for each ray during inference is computationally intensive. To address this, we implement a two-stage method. In the first stage, we identify the $k+1$ neural fields, where $k \geq 0$ represents the number of dynamic fields, that are likely to intersect with a given ray. The second stage involves rendering LiDAR measurements from these selected fields individually and then integrating them into a unified set of measurements.


\paragraph{Ray intersection test}
As outlined in~\cref{sec: decomposition}, each dynamic neural field is reconstructed in its unique canonical space, defined by a corresponding canonical box. To determine neural fields intersecting with a ray at inference time, we begin by estimating the transformations $\{\mathbf{T}_t^v\}_{v=1}^N$, which convert coordinates from the world framework to each vehicle's canonical space at timestamp $t$. These transformations are determined by interpolating the training set transformations using spherical linear interpolation (SLERP)~\cite{10.1145/325334.325242}. Following this, we apply transformations to the queried ray and run intersection tests with the canonical boxes of the scenes. 


\paragraph{Neural rendering from multiple neural fields}
 After identifying the $k+1$ neural fields that potentially intersect with a ray, we perform volume rendering on each field separately, yielding $k+1$ distinct sets of LiDAR measurements. Next, we evaluate the ray drop probabilities across these fields. A ray is deemed \textit{dropped} if all neural fields indicate a drop probability $\pdrop > 0.5$. For rays not classified as dropped, we sort the estimated ranges in ascending order and select the nearest one as our final range prediction. Correspondingly, the intensity value is extracted from the same neural field associated with this closest range.
