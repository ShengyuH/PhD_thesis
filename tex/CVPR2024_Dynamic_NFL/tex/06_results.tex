\section{Experiments}
\input{\subdir/fig/qualitative_waymodynamic}
\subsection{Datasets and evaluation protocol}\label{sec:datasets}

\paragraph{Real-world Dynamic scenes} 
We construct \textit{Waymo Dynamic} dataset by selecting four representative scenes from Waymo Open dataset~\cite{sun2020scalability}, with multiple moving vehicles inside. These scenes are comprised of sequences of 50 consecutive frames. For evaluation purposes, every fifth frame is designated for testing, while the other 40 frames are allocated for training.


\paragraph{Real-world static scenes}
We also evaluate our method on four static scenes as introduced in~\cite{Huang2023nfl}. There are two settings, \textit{Waymo Interp} applies the same evaluation protocol as \textit{Waymo Dynamic}, while \textit{Waymo NVS} employs a dedicated closed-loop evaluation to validate the real novel view synthesis performance. Please refer to NFL~\cite{Huang2023nfl} for more details about this setting. 
 


\paragraph{Synthetic static scenes}  
\textit{TownClean} and \textit{TownReal} are synthetic static scenes introduced in NFL~\cite{Huang2023nfl}. They consist of 50 LiDAR scans simulated in urban street environment, using non-diverging and diverging beams, respectively. 



\paragraph{Evaluation metrics}\label{sec:metrics}
To evaluate the LiDAR range accuracy, we employ a suite of four metrics: mean absolute errors~(MAE [cm]), median absolute errors~(MedAE [cm]), Chamfer distance~(CD[cm]) and MedAE for dynamic vehicles~(MedAE Dyn[cm]). For intensity evaluation, We report root mean square error~(RMSE).
%
In addition to our primary evaluations, we assess the re-simulated LiDAR scans' realism through two auxiliary tasks: object detection and semantic segmentation. For object detection, we calculate the \textit{detection agreement}~\cite{manivasagam2020lidarsim}, both for all vehicles (Agg.~[\%]) and specifically for dynamic vehicles (Dyn.$\;$Agg.~[\%]). Regarding semantic segmentation, we measure and report recall, precision, and the intersection over union (IoU[\%]). It's important to note that the predictions on the original LiDAR scans serve as our \textit{ground truth}, against which we compare the results obtained from the re-simulated scans.




\paragraph{Baseline methods}
Regarding LiDAR simulation on static scenes, NFL~\cite{Huang2023nfl} and LiDARsim\cite{manivasagam2020lidarsim} are two closest baselines to compare to. Additionally, we include i-NGP~\cite{mueller2022instant}, DS-NeRF~\cite{kangle2021dsnerf}, and URF~\cite{rematas2022urban} for comparison. As for simulation on dynamic scenes, we compare to LiDARsim~\cite{manivasagam2020lidarsim} and UniSim~\cite{yang2023unisim}\footnote{We re-implement LiDARsim~\cite{lee2015lidar} and UniSim~\cite{yang2023unisim} as they are not open-sourced.}. Please refer to the appendix for implementation details.


\input{\subdir/tables/quantitative_waymodynamic}
\input{\subdir/tables/quantitative_waymostatic}
\input{\subdir/fig/ecdf_waymodynamic}
\input{\subdir/fig/qualitative_waymostatic}
\subsection{LiDAR novel view synthesis evaluation} \label{sec:lidar_eval}
\paragraph{LiDAR NVS in dynamic scenes}
Quantitative comparisons with baseline methods are detailed in~\cref{tab:waymodynamic}. \dynfl notably outperforms LiDARsim~\cite{manivasagam2020lidarsim} and UniSim~\cite{yang2023unisim} in range reconstruction. This improvement is largely due to our SDF-based neural scene representation, which incorporates the physical aspects of LiDAR sensing. Additionally, our method employs a ray drop test when rendering multiple neural fields, leading to a more accurate reconstruction of dynamic vehicles, as evidenced in~\cref{fig:errormap_dynamic} and further supported by the data in~\cref{fig:ecdf}.


\input{\subdir/fig/sensor_manipulation}
\paragraph{LiDAR NVS in static scenes}
In addition to dynamic scenes, we evaluate \dynfl against baseline methods in static scenarios, with the results detailed in~\cref{tab:waymostatic} and~\cref{fig:error_map}. \dynfl excels in reconstructing geometry in most cases. A key observation is its superior performance in reconstructing planar regions (\eg the ground shown in~\cref{fig:error_map}), especially when compared to NFL~\cite{Huang2023nfl}, which also uses a neural field for surface representation. This improvement is largely due to the enhanced surface regularizations provided by our advanced SDF-based surface modeling approach.


\input{\subdir/tables/ablation_activesensing}
\input{\subdir/tables/ablation_surfacesdf}
\input{\subdir/fig/ablation_raydrop}

\input{\subdir/fig/detection_qualitative}
\input{\subdir/tables/detection_agreement}
\input{\subdir/tables/seg_nvs}




\subsection{Ablation study}
\paragraph{SDF-based volume rendering for active sensing}
We begin by assessing the efficacy of our SDF-based volume rendering for active sensor, the results are shown in~\cref{tab:active_sensing}. When compared to our baseline that uses the SDF-based volume rendering for passive sensing, \dynfl demonstrates enhanced performance in both synthetic (\textit{TownClean}) and real-world (\textit{Waymo Interp} and \textit{Waymo Dynamic}) datasets, indicating the importance of incorporating the physical sensing process of LiDAR in addressing the inverse problem.


\paragraph{Neural fields composition} 
To validate the efficacy of our two-stage neural field composition approach, we compare it with an alternative approach utilized in UniSim~\cite{yang2023unisim}. The results are shown in~\cref{tab:waymodynamic}. UniSim~\cite{yang2023unisim} blends different neural fields by sampling points from all intersected neural fields, followed by a single evaluation of volume rendering to produce the final LiDAR scan. In contrast, our method independently renders from each intersecting neural field first, and then combines these measurements into a final measurement using a ray drop test (\cf~\cref{fig:ablation_raydrop}). This approach leads to a notable improvement in geometry reconstruction over UniSim~\cite{yang2023unisim}, exemplified by our method halving the Median Absolute Error (MedAE) across all points. This enhancement is even more evident when focusing solely on points related to dynamic vehicles (\cf~\cref{fig:ecdf}).
% 

\paragraph{Surface points' SDF constraint}
We examine the importance of the surface points' SDF constraint discussed in ~\cref{sec:optmisation} on \textit{Town Real} and \textit{Waymo Interp} datasets. The results shown in \cref{tab:surface_sdf} suggest that our method yields improved geometry reconstruction quality by additionally enforcing LiDAR points to have zero SDF values. 


\subsection{Auxiliary task evaluations} 
\label{sec:downstream}
To assess the fidelity of our neural re-simulation and gauge the domain gap between re-simulated and real scans, we evaluate their applicability in two downstream tasks: object detection and semantic segmentation.


\paragraph{Object detection}
We utilize the pre-trained FSDv2~\cite{fan2023fsdv2} model for object detection and conduct evaluations on the re-simulated LiDAR scans within the \textit{Waymo Dynamic} dataset. Our results are compared against those from LiDARsim~\cite{manivasagam2020lidarsim}, with the findings detailed in~\cref{tab:detection} and~\cref{fig:detection}. Notably, \dynfl exhibits a more substantial detection agreement with the predictions on real LiDAR scans. This indicates a higher fidelity in our re-simulations and a reduced domain gap relative to actual scans.


\paragraph{Semantic segmentation}
For semantic segmentation, we use the pre-trained SPVNAS model~\cite{tang2020searching}, with the results presented in~\cref{tab:sem_seg_nvs_ours}. \dynfl improves over baseline methods according to most evaluation metrics, underscoring the realism of our re-simulated LiDAR scans.



\subsection{Scene editing}
Beyond LiDAR novel view synthesis by adjusting the sensor configurations (\cf \cref{fig:lidar_nvs}), we additionally demonstrate the practicality of our compositional neural fields approach through two scene editing applications.

\input{\subdir/fig/object_insertion}
\paragraph{Insert object from one scene into another}
Our explicit neural scene de-composition and flexible composition technique enable seamless insertion and removal of neural assets across scenes. As demonstrated in~\cref{fig:vehicle_insertion}, we are able to replace a car from one scene with a truck from another scene, achieving accurate reconstruction of both geometry and intensity. In contrast, UniSim~\cite{yang2023unisim} struggles to preserve high quality geometry. This highlights the significant potential of our approach in generating diverse and realistic LiDAR scans for autonomous driving scenarios.

\input{\subdir/fig/trajectory_manipulation}
\paragraph{Manipulate the trajectory of dynamic objects}
\dynfl also facilitates the manipulation of moving objects' trajectories by simply adjusting their relative poses to the canonical bounding box. Representative results are shown in ~\cref{fig:traj}. The high realism of our re-simulation is also indicated by the successful detection of inserted virtual objects.
