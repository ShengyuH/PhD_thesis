\section{Related Work}
\label{sec:related}

\paragraph{Temporal point cloud processing}
Modeling a sequence of point clouds usually starts with estimating accurate correspondences between frames, for which scene flow emerged as a popular representation. Originating from \cite{vedula1999three}, scene flow estimation builds an intuitive and effective dynamic scene representation by computing a flow vector for each source point. While traditional scene flow methods~\cite{wedel2008efficient,vogel20113d,vogel2013piecewise,vogel20153d} leverage motion smoothness as regularizer within their optimization frameworks, modern learning-based methods learn the preference for smooth motions directly from large-scale datasets~\cite{liu2019flownet3d,wu2019pointpwc,puy2020flot,ouyang2021occlusion}. Moreover, manually designed scene priors proved beneficial for structured scenes, for instance supervoxel-based local rigidity constraints~\cite{li2021hcrf}, or object-level shape priors learned in fully supervised~\cite{behl2019pointflownet} or weakly supervised~\cite{gojcic2021weakly} fashion.
Methods like SLIM~\cite{baur2021slim} take a decoupled approach, where they first run motion segmentation before deriving scene flows for each segment separately.
Treating the entire point cloud sequence as 4D data and applying a spatio-temporal backbone~\cite{liu2019meteornet,choy2019Minkowski,fan2020pstnet} demonstrates superior performance and efficiency. Subsequent works enhance such backbones by employing long-range modeling techniques such as Transformers~\cite{vaswani2017attention,fan2021point,yang20213d}, or by coupling downstream tasks like semantic segmentation~\cite{aygun20214d}, object detection~\cite{yang2021auto4d,qi2021offboard} and multi-modal fusion~\cite{piergiovanni20214d}.
While our method employs the representation of multi-frame scene flow, we explicitly model individual dynamic objects, which not only provides a high-level scene decomposition, but also yields markedly higher accuracy.

\paragraph{Motion segmentation} 
Classification of the points into static and dynamic scene parts serves as an essential component in our pipeline.
Conventional geometric approaches either rely on ray casting~\cite{chen2016dynamic,schauer2018peopleremover} over dense terrestrial laser scans to build clean static maps, or on visibility~\cite{pomerleau2014long,kim2020remove} to determine the dynamics of the query point by checking its occlusion state in a dense map.
Removert~\cite{kim2020remove} iteratively recovers falsely removed static points from multi-scale range images. Most recently, learning-based methods formulate and solve the segmentation task in a data-driven way: Chen \etal~\cite{chen2021ral} propose a deep model over multiple range image residuals and show SoTA results on a newly-established motion segmentation benchmark~\cite{behley2019semantickitti}. Any Motion Detector~\cite{filatov2020any} first extracts per-frame features from bird's-eye-view projections and then aggregates temporal information from ego-motion compensated per-frame features (in their case with a with convolutional RNN). Our work is similar in spirit, but additionally leverages information from the foreground segmentation task and object clustering in an end-to-end framework.

\paragraph{Dynamic object reconstruction} 
Given sequential observations of a rigid object, dynamic object reconstruction aims to recover the 3D geometric shape as well as  its rigid pose over time. Such a task can be handled either by directly hallucinating the full shape or by registering and accumulating partial observations. Approaches of the former type usually squash partial observations into a global feature vector~\cite{yuan2018pcn,giancola2019leveraging,li2019pu} and ignore the local geometric structure. \cite{gu2020weakly} go one step further by disentangling shape and pose with a novel supervised loss. However, there is still no guarantee for the fidelity of the completed shape. We instead rely on registration and accumulation. Related works include AlignNet-3D~\cite{gross2019alignnet} that directly regresses the relative transformation matrix from concatenated global features of the two point clouds. NOCS~\cite{wang2019normalized} proposes a category-aware canonical representation that can be used to estimate instance pose \wrt its canonical pose. Caspr~\cite{rempe2020caspr} implicitly accumulates the shapes by mapping a sequence of partial observations to a continuous latent space. \cite{huang2021multibodysync} and \cite{huang2022multiway} respectively propose multi-way registration methods that accumulate multi-body and non-rigid dynamic point clouds, but do not scale well to large scenes.