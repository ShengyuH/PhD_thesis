\section{Experimental Evaluation}
\label{sec:results}
In this section, we first describe the datasets and the evaluation setting for our experiments (\cref{sec:dataset_evalsetting}). We then proceed with a quantitative evaluation of our method and showcase its applicability to downstream tasks with qualitative results for surface reconstruction~(\cref{sec:evaluation_results}). Finally, we validate our design choices in an ablation study~(\cref{sec:ablation}).

\subsection{Datasets and Evaluation Setting}
\label{sec:dataset_evalsetting}

\paragraph{Waymo} The Waymo Open Dataset~\cite{sun2020scalability} includes 798/202 scenes for training/validation, where each scene is a 20-second clip captured by a 64-beam LiDAR at 10~Hz. We randomly sample 573/201 scenes for training/validation from the training split, and treat the whole validation split as a held-out test set. We consider every 5 consecutive frames as a \textit{sample} and extract 20 \textit{samples} from each clip, for a total of 11440/4013/4031 samples for training/validation/test. %
    
\paragraph{nuScenes} The nuScenes dataset~\cite{caesar2020nuscenes} consists of 700 training and 150 validation scenes, where each scene is a 20-second clip captured by a 32-beam LiDAR at 20$\,$Hz. We use all validation scenes for testing and randomly hold out 150 training scenes for validation. We consider every 11 consecutive frames as a \textit{sample}, resulting in a total of 10921/2973/2973 samples for training/validation/testing.

\paragraph{Ground truth} We follow~\cite{jund2021scalable} to construct pseudo ground-truth from the detection and tracking annotations. Specifically, the flow vectors of the background part are obtained from ground truth ego-motion. For foreground objects, we use the
unique instance IDs of the tracking annotations and recover their rigid motion parameters by registering the bounding boxes.
Notably, for \nuscenes~the bounding boxes are only annotated every 10 frames. To obtain pseudo ground truth for the remaining frames, we linearly interpolate the boxes, which may introduce a small amount of label noise especially for fast-moving objects.

\input{\subdir/tab/sf_results}
\paragraph{Metrics}
We use standard \emph{scene flow} evaluation metrics~\cite{liu2019flownet3d,baur2021slim} to compare the performance of our approach to the selected baselines. These metric include:
\textit{(i)} 3D end-point-error~(\textit{EPE}~[m]) which denotes the mean $L_2$-error of all flow vectors averaged over all frames; 
\textit{(ii)} strict/relaxed accuracy (\textit{AccS}~[\%]~/\textit{AccR}~[\%]). \ie, the fraction of points with \textit{EPE} $<$ 0.05/0.10m or relative error $<$ 0.05/0.10; \textit{(iii)} \textit{Outliers~[\%]} which denotes the ratio of points with \textit{EPE} $>$ 0.30m or relative error $>$ 0.10; and \textit{(iv)} \textit{ROutliers}~[\%], the fraction of points whose \textit{EPE} $>$ 0.30m and relative error $>$ 0.30. We evaluate these metrics for the static and dynamic parts of the scene separately.\footnote{A point is labelled as \textit{dynamic} if its ground-truth velocity is $>0.5\,$m/s.} %
Following \cite{wu2020motionnet,luo2021self}, we evaluate the performance of all methods only on the points that lie within the square of size $64\times 64$ m$^2$ centered at the ego-car location in the target frame. Additionally we remove ground points by thresholding along the $z$-axis.\footnote{This setting turns out to better suit the baseline methods~\cite{puy2020flot,wu2019pointpwc,li2021neural}. However, we keep ground points in our dynamic point cloud accumulation task, as thresholding could falsely remove points that are of interest for reconstruction or mapping.} Ablations studies additionally report the quality of \textit{motion segmentation} in terms of recall and precision of \textit{dynamic} parts, and the quality of \textit{spatio-temporal instance association} in terms of mean weighted coverage (\textit{mWCov}, the \textit{IoU} of recovered instances~\cite{wang2019associatively}). For further details, see the appendix.

\paragraph{Baselines}
We compare our method to 4 baseline methods. PPWC~\cite{wu2019pointpwc} and FLOT~\cite{puy2020flot} are based on dense matching and are trained in a fully supervised manner; WsRSF~\cite{gojcic2021weakly} assumes a \textit{multi-body} scene and can be trained with weak supervision; NSFPrior~\cite{li2021neural} is an optimisation-based method without pre-training. For PPWC~\cite{wu2019pointpwc}, FLOT~\cite{puy2020flot} and WsRSF~\cite{gojcic2021weakly}, we sample at most 8192 points from each frame due to memory constraints, and use the $k$-nn graph to up-sample flow vectors to full resolution at inference time. For NSFPrior~\cite{li2021neural}, we use the full point clouds and take the default hyper-parameter settings given by the authors, except for the early-stopping patience, which we set to 50 to make it computationally tractable on our large-scale dataset. For all baseline methods, we directly estimate flow vectors from any \textit{source} frame to the \textit{target} frame. 

\subsection{Main Results}
\label{sec:evaluation_results}
The detailed comparison on the \waymo~and \nuscenes~data is given in \cref{tab:sf_main}. \emph{Ours} denotes the direct output of our model, while \emph{Ours+} describes the results after test-time refinement with ICP. Many downstream tasks (e.g., surface reconstruction) rely on the accumulation of full point clouds and require also ground points. We therefore also train a variant of our method on point clouds with ground points and denote it \emph{Ours (w. ground)}. To facilitate a fair comparison, we use full point clouds as input during training and inference, but only compute the evaluation metrics on points that do not belong to ground. 

\paragraph{Comparison to state of the art}
\input{\subdir/figs/tex/percentile}
On the static part of the \textbf{\waymo}~dataset, FLOT~\cite{puy2020flot} reaches the best performance among the baselines, but still has an average EPE error of 12.9$\,$cm, which is more than 4 times larger than that of \textit{Ours} (2.8$\,$cm). This result corroborates our motivation for decomposing the scene into a static background and dynamic foreground. Modeling the motion of the background with a single set of transformation parameters also enables us to run ICP at test time (\textit{Ours+}), which further reduces the EPE of the static part to 1.8$\,$cm. On the dynamic foreground points, NSFPrior~\cite{li2021neural} reaches a comparable performance to \textit{Ours}. However, based on our spatio-temporal association of foreground points we can again run ICP at test-time, which reduces the median EPE error to 4.3$\,$cm, $\approx$40$\,\%$ lower than that of NSFPrior. Furthermore, NSFPrior is an optimization method and not amenable to online processing (see Tab.~\ref{tab:runtime}). The results for \textbf{\nuscenes} follow a similar trend as the ones for \emph{Waymo}, but are larger in absolute terms due to the lower point density. Our method achieves the best performance on both static and dynamic parts. The gap to the closest competitors is even larger, which auggests that our method is more robust to low point density.

To further understand the error distribution of the dynamic parts, and the evolution of the errors of the static part as the gap between the \textit{source} and \textit{target} frames increases, we plot detailed results in \cref{fig:epe3d_ecdf}. On both \waymo~and \nuscenes{} our method degrades gracefully, and slower than the baselines. The ECDF curve of the EPE error for foreground points also shows that our method performs best at all thresholds. 

\paragraph{Breakdown of the performance gain}
Overall, our method improves over baseline methods by a large margin on two datasets. The gains are a direct result of our design choices: \textit{(i)}~by modelling the flow of the background as rigid motion instead of unconstrained flow, we can greatly reduce \emph{ROutlier} and improve the accuracy; \textit{(ii)}~different from~\cite{gojcic2021weakly} we perform motion segmentation, and can thus assign ego-motion flow to points on movable, but \emph{static} objects ($\approx75\%$ of the foreground). This further improves the results (see EPE of static FG in \cref{tab:ablations}); \textit{(iii)}~reasoning on the object level, combined with spatio-temporal association and modelling, improves flow estimates for the \textit{dynamic} foreground.

\paragraph{Generalisation to variable input length $T$}
\label{sec: more frames}
When trained with a fixed input length ($T=5$ on \emph{Waymo}), our model is able to generalize to different input lengths (see Tab.~\ref{tab:generalisation}). The performance degrades moderately with increasing $T$, as the motions become larger than seen during training. Also, larger displacements make the correspondence problem inherently harder.
\input{\subdir/tab/generalisation}

\input{\subdir/tab/runtime}
\paragraph{Runtime}
We report runtimes for our model and several baseline methods on both datasets in \cref{tab:runtime}~(left). Our method is significantly faster than all baselines under the multi-frame scene flow setting. We also report detailed runtimes of our model for individual steps in \cref{tab:runtime}~(right). As we can see, backbone feature extraction and pairwise registration (\textit{ego-motion estimation}) account for the majority of the runtime, 57.5\% on \waymo~and 75.2\% on \nuscenes. Note that this runtime is calculated over all frames, while under a data streaming setting, we only need to run the first part for a single incoming frame, then re-use the features of the previous frames at later stages, which will greatly reduce runtime: after initialisation, the runtime for every new sample decreases to around 0.094$\,$s for \waymo, respectively 0.079$\,$s on \nuscenes. 

\paragraph{Qualitative results}
\input{\subdir/figs/tex/qualitative_results}
In~\cref{fig:qualitative_results} we show qualitative examples of scene and object reconstruction with our approach. By jointly estimating the ego-motion of the static part and the moving object motions, our method accumulates the corresponding points into a common, motion-compensated frame. It thus provides an excellent basis for 3D surface reconstruction.

\input{\subdir/tab/ablations}
\subsection{Ablation Study}
\label{sec:ablation}
\paragraph{Sequential model}
We evaluate the individual modules of our sequential model, namely the foreground segmentation (\textit{FG}), motion segmentation (\textit{MOS}) and offset compensation (\textit{Offset}). We train two models with and without foreground segmentation. For variants without \textit{MOS} or \textit{Offset}, we take the trained full model but remove \textit{MOS} or \textit{Offset} at inference time. The detailed results are summarised in \cref{tab:ablations}. \textit{FG} enables us to exclude dynamic foreground objects during pairwise registration. On \waymo, this reduces EPE of the static parts by $30\%$ from 4.1 to 2.9$\,$cm, and as a result also reduces EPE of dynamic parts from 28.6 to 19.7$\,$cm. By additionally extracting the static foreground parts with \textit{MOS}, the model can recover more accurate ego-motion for them, which reduces EPE from 19.0/19.9 to 2.1/7.4$\,$cm on \waymo/\nuscenes. \textit{Offset} robustifies the instance association against low point density and fast object motion (+3.1 $pp$ in \textit{WCov} on \nuscenes), this further reduces the EPE of dynamic parts by 3.7$\,$cm. 

\paragraph{Ego-motion estimation strategy}
By default, we directly estimate the ego-motion from any \textit{source} frame to the \textit{target} frame. We compare to an alternative which estimates the ego-motion relative to the previous frame. Although that achieves smaller pairwise errors, after chaining the estimated poses the errors w.r.t.\ the \textit{target} frame explode, resulting in inferior scene flow estimates (\textit{chained poses} in \cref{tab:ablations}).

\paragraph{Comparison to tracking-based method}
Instead of running spatio-temporal association followed by TubeNet to model the motion of each moving object, an alternative would be to apply Kalman tracker so as to simultaneously solve association and motion. We compare to the modified AB3DMOT~\cite{weng20203d}, which is based on a constant velocity model. That method first clusters moving points for each frame independently to obtain instances, then associates instances by greedy matching of instance centroids based on $L_2$ distances. The results in \cref{tab:ablations} (\textit{Kalman tracker}) show clearly weaker performance, due to the less robust proximity metric based on distances between noisy centroid estimates. 