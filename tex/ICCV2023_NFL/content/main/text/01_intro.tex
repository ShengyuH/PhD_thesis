\section{Introduction}
\label{sec:intro}

The goal of novel view synthesis is to generate a view of a 3D scene, from a viewpoint at which no real sensor image has been captured. This offers the possibility to observe \emph{real} scenes from a \emph{virtual}, unobserved perspective. Among other applications, it has tremendous potential for autonomous driving: synthetic novel views may be used to train and test perception algorithms across a wider range of viewing conditions, thus enhancing robustness and generalization. Moreover, novel view synthesis becomes critical when the desired viewpoints are not known in advance, \eg, during training of a planning module whose decisions determine future vehicle locations.

Neural radiance fields (NeRFs) have led to unprecedented visual quality when synthesizing novel camera views~\cite{mildenhall2020nerf, barron2021mip, yu2021plenoxels, muller2022instant}. These methods represent the 3D scene in form of continuous density and radiance fields, from which images can be generated through volume rendering, mimicking the image acquisition process. The inductive bias of neural networks imparts NeRFs the ability to interpolate complex lighting and reflectance behaviours with a high degree of realism.

While most prior works focused on synthesizing camera views, 3D perception in the autonomous driving context typically relies partly (or even exclusively) on LiDAR measurements. Synthesizing realistic LiDAR scans from novel viewpoints thus has a lot of potential for data augmentation and closed-loop testing of autonomous navigation systems.

The problem of synthesizing novel LiDAR views has previously been addressed in two stages~\cite{manivasagam2020lidarsim}. First, extract an explicit surface representation such as surfels or a triangular mesh from the scanned point clouds. Then, simulate LiDAR measurements from a novel viewpoint by casting rays and intersecting them with the surface model.
Like for images, explicit reconstruction (which is not optimised towards the subsequent synthesis step) suffers from discretization artifacts and introduces noticeable errors~\cite{waechter2014let}. Moreover, the rendering assumes an idealised ray model and neglects the divergence of the LiDAR beams, which causes frequent second returns from distant surfaces. 

Here, we instead build on a main insight of NeRF~\cite{mildenhall2020nerf}: directly optimizing an implicit scene representation for novel view synthesis can produce more realistic outputs than the reconstruct-then-simulate approach. Specifically, we propose Neural Fields for LiDAR (NFL), a NeRF-style representation for synthesizing novel LiDAR viewpoints.

Several NeRF extensions have utilized range measurements as additional supervision, and have shown that constraining the scene geometry more tightly can yield better (camera) view synthesis~\cite{deng2021depth,rematas2021urban}. Yet, the output of those methods are synthetic images, not LiDAR scans, consequently they have not paid attention to effects specific to LiDAR sensing: a laser scanner does \emph{not} directly sense range, rather it measures the returned light energy per ray and determines the range based on the waveform. 
This includes the possibilities that there are multiple returns\footnote{In principle there can be \textgreater2 returns, but automotive LiDAR sensors typically record the first two echos.} from the emitted ray, or no return at all.

Our formulation closely adheres to the principles of the LiDAR measurement process 
and incorporates them into the neural field framework. Specifically, we (\romannumeral 1)~\textbf{devise volume rendering for LiDAR sensors};  
(\romannumeral 2)~\textbf{incorporate beam divergence} and (\romannumeral 3)~\textbf{propose truncated volume rendering} to account for secondary returns and improve range prediction. 

We evaluate our method on both synthetic and real LiDAR data. To this end, we (\romannumeral 4)~\textbf{develop a LiDAR simulator} for synthesizing scenes from 3D assets that serve as a test bed for viewpoints far from the original scan locations, and to study the effect of different scan patterns.
Real data from the Waymo~\cite{sun2020scalability} dataset is used to evaluate NFL against real scans at held-out viewpoints, including real-world intensities, ray drops and secondary returns. Additionally, we (\romannumeral 5)~\textbf{propose a novel closed-loop evaluation protocol} that leverages real data to evaluate view synthesis in challenging views.
As an end-to-end test for downstream tasks, we further evaluate the performance of state-of-the-art segmentation and registration networks when trained on real scans and tested on novel views generated by NFL.



