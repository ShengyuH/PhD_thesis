\section{Related Work}
\label{sec:relwork}
\paragraph{LiDAR simulation}
Simulating realistic LiDAR data is useful for training perception models. Different from real-world LiDAR data that requires annotation efforts, simulated data can be automatically generated with ground truth labels, \eg object bounding boxes and semantic segmentation. Unrealistic LiDAR simulation will prevent the trained models from generalizing to real data.   %
Traditional simulation engines, such as those proposed in~\cite{dosovitskiy2017carla,koenig2004design}, require the specification of sensor parameters and 3D scene assets and use ray-casting methods for simulation. 
 Although these point clouds can accurately represent scene geometry, they often exhibit a discrepancy, or "domain gap", compared to real data, due to the lack of modeling for sensor noise, such as ray drop and Gaussian beam. 
Furthermore, this approach relies heavily on the creation of 3D scene assets, which can be time-consuming and expensive. 
To address these challenges, LiDARsim~\cite{manivasagam2020lidarsim} reconstructs the static and dynamic scene assets from real data using surfel~\cite{pfister2000surfels} representation and models the ray-drop pattern for improved realism. BaiduSim~\cite{baidusim} proposes a probability map to model scene compositions in order to reduce the domain gap. Most recent work~\cite{guillard2022learning} learns to enhance existing simulated LiDAR intensity and ray-drop patterns, using the available corresponding RGB images. 

Weather conditions, such as fog or rain, can significantly impact the quality of LiDAR data, and downstream models trained solely on ideal weather conditions may fail to generalize to these effects. Recent methods and datasets \cite{hahner2021fog,hahner2022lidar,kilic2021lidar,bijelic2020seeing} have been proposed to address this issue. %
  SnowSIM~\cite{hahner2022lidar} and FogSIM~\cite{hahner2021fog} sample snow particles and model the impulse response from atmospheric attenuation, respectively, to alter the range measurements of each ray.
 Other approaches~\cite{shih2022reconstruction,kilic2021lidar, kurup2021dsor} simulate LiDAR data on rainy days and the spray effects, in a similar fashion.


\paragraph{NeRF for Novel View Synthesis}
NeRF~\cite{mildenhall2020nerf} maps 5D position and direction to density and radiance scene values, and uses volume rendering~\cite{max1995optical,max2005local} to estimate pixel color. This technique has proven effective for generating realistic images at unseen camera views. Many methods have been proposed to improve robustness to camera poses~\cite{lin2021barf,chng2022garf}, handle dynamics~\cite{ost2021neural,pumarola2021d}, anti-alias~\cite{zhang2020nerf++,barron2021mip,barron2022mip}, and speed up optimisation~\cite{liu2020neural,yu2021plenoxels,mueller2022instant} \etc. Despite its high-quality novel view synthesis capacity, the underlying geometry of NeRF is considered inaccurate and noisy~\cite{oechsle2021unisurf}, making it less favoured for geometry reconstruction, especially in sparse-views settings. ~\cite{oechsle2021unisurf,yariv2021volume,wang2021neus} address this challenge by using implicit surface representations, and defining the density functions based on them to enabling volume rendering. DS-NeRF~\cite{deng2021depth} and DenseDS-NeRF~\cite{roessle2021dense} use sparse depth supervision from SfM~\cite{schoenberger2016sfm} points to regularise the density field. Urban Radiance Field~\cite{rematas2021urban} leverages LiDAR data for depth supervision. %

\paragraph{Neural fields beyond regular cameras}
Neural fields are a natural and continuous representation~\cite{xie2022neural} for spatio-temporal information including SDFs~\cite{park2019deepsdf}, occupancy~\cite{mescheder2019occupancy} and radiance field \cite{mildenhall2020nerf} \etc. 
While in its original form, NeRF~\cite{mildenhall2020nerf} performs novel view synthesis using tonemapped low dynamic range images, RawNeRF~\cite{mildenhall2022nerf} extends it to operate over the high dynamic range images, enabling additional adjustments to focus, exposure, and tonemapping. T{\"o}rf~\cite{attal2021torf} incorporates the image formation model for continuous-wave Time-of-Flight (ToF) cameras into NeRF, allowing it to jointly process RGB and ToF sensor data and improve reconstruction robustness to large motions. EventNeRF~\cite{rudnev2022eventnerf} and ENeRF~\cite{klenk2022nerf} optimise the scene representation for Novel View Synthesis (NVS) from sparse event streams that contain asynchronous per-pixel brightness change signals. Other works~\cite{qadri2022neural,luo2022learning} explore the use of acoustic signals for surface reconstruction or NVS. 